{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-06-20T12:22:00.793954Z",
     "end_time": "2023-06-20T12:22:03.236184Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import glob\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "        subjid  label  shape_Elongation  shape_Flatness   \n0       Oslo01      1          0.829356        0.682981  \\\n1       Oslo01      2          0.944143        0.900681   \n4       Oslo02      4          0.849295        0.841082   \n5       Oslo02      5          0.939828        0.821847   \n7       Oslo03      1          0.930698        0.629604   \n...        ...    ...               ...             ...   \n2408  Stan_328      1          0.810571        0.325076   \n2415  Stan_328      8          0.967496        0.597282   \n2418  Stan_328     11          0.561746        0.506390   \n2419  Stan_328     12          0.959775        0.756903   \n2427  Stan_338      4          0.819799        0.618051   \n\n      shape_LeastAxisLength  shape_MajorAxisLength   \n0                  8.512717              12.464063  \\\n1                 14.133927              15.692487   \n4                  8.703783              10.348315   \n5                 12.093804              14.715396   \n7                 10.914986              17.336268   \n...                     ...                    ...   \n2408               8.769420              26.976501   \n2415               6.131668              10.265944   \n2418               5.004740               9.883181   \n2419               5.164755               6.823538   \n2427               8.548537              13.831433   \n\n      shape_Maximum2DDiameterColumn  shape_Maximum2DDiameterRow   \n0                         14.866069                   14.560220  \\\n1                         17.804494                   19.416488   \n4                         10.816654                   11.704700   \n5                         17.262677                   17.888544   \n7                         20.615528                   18.439089   \n...                             ...                         ...   \n2408                      31.384710                   28.160256   \n2415                      11.661904                   12.369317   \n2418                       8.246211                   11.704700   \n2419                       7.615773                    8.602325   \n2427                      14.142136                   15.297059   \n\n      shape_Maximum2DDiameterSlice  shape_Maximum3DDiameter  ...   \n0                        12.529964                15.394804  ...  \\\n1                        18.681542                19.467922  ...   \n4                        11.704700                12.449900  ...   \n5                        17.492856                18.000000  ...   \n7                        19.416488                20.712315  ...   \n...                            ...                      ...  ...   \n2408                     27.459060                33.075671  ...   \n2415                     12.165525                12.688578  ...   \n2418                     10.000000                12.124356  ...   \n2419                      8.602325                 8.602325  ...   \n2427                     17.204651                17.291616  ...   \n\n      ngtdm_Busyness  ngtdm_Coarseness  ngtdm_Complexity  ngtdm_Contrast   \n0         158.977235          0.008929          0.302273        0.074659  \\\n1          39.445794          0.007407          0.115448        0.010426   \n4          14.239428          0.022868          0.159307        0.020316   \n5          49.609354          0.008536          0.137901        0.028455   \n7          43.648474          0.007781          0.123220        0.017415   \n...              ...               ...               ...             ...   \n2408       12.988427          0.019891          0.030104        0.000639   \n2415       18.452502          0.024791          0.208465        0.046975   \n2418        0.553323          0.468447          0.025263        0.000748   \n2419        0.000000    1000000.000000          0.000000        0.000000   \n2427        6.977967          0.040814          0.057854        0.005610   \n\n      ngtdm_Strength  Age  number_annotations  F  M  labels  \n0           0.008950   58                   1  0  1       0  \n1           0.007130   58                   1  0  1       0  \n4           0.023465   50                   1  1  0       1  \n5           0.008212   50                   1  1  0       1  \n7           0.007982   64                   1  0  1       0  \n...              ...  ...                 ... .. ..     ...  \n2408        0.019768   53                   1  0  1       1  \n2415        0.023205   53                   1  0  1       1  \n2418        0.365681   53                   1  0  1       1  \n2419        0.000000   53                   1  0  1       1  \n2427        0.031493   42                   1  1  0       1  \n\n[438 rows x 114 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subjid</th>\n      <th>label</th>\n      <th>shape_Elongation</th>\n      <th>shape_Flatness</th>\n      <th>shape_LeastAxisLength</th>\n      <th>shape_MajorAxisLength</th>\n      <th>shape_Maximum2DDiameterColumn</th>\n      <th>shape_Maximum2DDiameterRow</th>\n      <th>shape_Maximum2DDiameterSlice</th>\n      <th>shape_Maximum3DDiameter</th>\n      <th>...</th>\n      <th>ngtdm_Busyness</th>\n      <th>ngtdm_Coarseness</th>\n      <th>ngtdm_Complexity</th>\n      <th>ngtdm_Contrast</th>\n      <th>ngtdm_Strength</th>\n      <th>Age</th>\n      <th>number_annotations</th>\n      <th>F</th>\n      <th>M</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Oslo01</td>\n      <td>1</td>\n      <td>0.829356</td>\n      <td>0.682981</td>\n      <td>8.512717</td>\n      <td>12.464063</td>\n      <td>14.866069</td>\n      <td>14.560220</td>\n      <td>12.529964</td>\n      <td>15.394804</td>\n      <td>...</td>\n      <td>158.977235</td>\n      <td>0.008929</td>\n      <td>0.302273</td>\n      <td>0.074659</td>\n      <td>0.008950</td>\n      <td>58</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Oslo01</td>\n      <td>2</td>\n      <td>0.944143</td>\n      <td>0.900681</td>\n      <td>14.133927</td>\n      <td>15.692487</td>\n      <td>17.804494</td>\n      <td>19.416488</td>\n      <td>18.681542</td>\n      <td>19.467922</td>\n      <td>...</td>\n      <td>39.445794</td>\n      <td>0.007407</td>\n      <td>0.115448</td>\n      <td>0.010426</td>\n      <td>0.007130</td>\n      <td>58</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Oslo02</td>\n      <td>4</td>\n      <td>0.849295</td>\n      <td>0.841082</td>\n      <td>8.703783</td>\n      <td>10.348315</td>\n      <td>10.816654</td>\n      <td>11.704700</td>\n      <td>11.704700</td>\n      <td>12.449900</td>\n      <td>...</td>\n      <td>14.239428</td>\n      <td>0.022868</td>\n      <td>0.159307</td>\n      <td>0.020316</td>\n      <td>0.023465</td>\n      <td>50</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Oslo02</td>\n      <td>5</td>\n      <td>0.939828</td>\n      <td>0.821847</td>\n      <td>12.093804</td>\n      <td>14.715396</td>\n      <td>17.262677</td>\n      <td>17.888544</td>\n      <td>17.492856</td>\n      <td>18.000000</td>\n      <td>...</td>\n      <td>49.609354</td>\n      <td>0.008536</td>\n      <td>0.137901</td>\n      <td>0.028455</td>\n      <td>0.008212</td>\n      <td>50</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Oslo03</td>\n      <td>1</td>\n      <td>0.930698</td>\n      <td>0.629604</td>\n      <td>10.914986</td>\n      <td>17.336268</td>\n      <td>20.615528</td>\n      <td>18.439089</td>\n      <td>19.416488</td>\n      <td>20.712315</td>\n      <td>...</td>\n      <td>43.648474</td>\n      <td>0.007781</td>\n      <td>0.123220</td>\n      <td>0.017415</td>\n      <td>0.007982</td>\n      <td>64</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2408</th>\n      <td>Stan_328</td>\n      <td>1</td>\n      <td>0.810571</td>\n      <td>0.325076</td>\n      <td>8.769420</td>\n      <td>26.976501</td>\n      <td>31.384710</td>\n      <td>28.160256</td>\n      <td>27.459060</td>\n      <td>33.075671</td>\n      <td>...</td>\n      <td>12.988427</td>\n      <td>0.019891</td>\n      <td>0.030104</td>\n      <td>0.000639</td>\n      <td>0.019768</td>\n      <td>53</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2415</th>\n      <td>Stan_328</td>\n      <td>8</td>\n      <td>0.967496</td>\n      <td>0.597282</td>\n      <td>6.131668</td>\n      <td>10.265944</td>\n      <td>11.661904</td>\n      <td>12.369317</td>\n      <td>12.165525</td>\n      <td>12.688578</td>\n      <td>...</td>\n      <td>18.452502</td>\n      <td>0.024791</td>\n      <td>0.208465</td>\n      <td>0.046975</td>\n      <td>0.023205</td>\n      <td>53</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2418</th>\n      <td>Stan_328</td>\n      <td>11</td>\n      <td>0.561746</td>\n      <td>0.506390</td>\n      <td>5.004740</td>\n      <td>9.883181</td>\n      <td>8.246211</td>\n      <td>11.704700</td>\n      <td>10.000000</td>\n      <td>12.124356</td>\n      <td>...</td>\n      <td>0.553323</td>\n      <td>0.468447</td>\n      <td>0.025263</td>\n      <td>0.000748</td>\n      <td>0.365681</td>\n      <td>53</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2419</th>\n      <td>Stan_328</td>\n      <td>12</td>\n      <td>0.959775</td>\n      <td>0.756903</td>\n      <td>5.164755</td>\n      <td>6.823538</td>\n      <td>7.615773</td>\n      <td>8.602325</td>\n      <td>8.602325</td>\n      <td>8.602325</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>1000000.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>53</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2427</th>\n      <td>Stan_338</td>\n      <td>4</td>\n      <td>0.819799</td>\n      <td>0.618051</td>\n      <td>8.548537</td>\n      <td>13.831433</td>\n      <td>14.142136</td>\n      <td>15.297059</td>\n      <td>17.204651</td>\n      <td>17.291616</td>\n      <td>...</td>\n      <td>6.977967</td>\n      <td>0.040814</td>\n      <td>0.057854</td>\n      <td>0.005610</td>\n      <td>0.031493</td>\n      <td>42</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>438 rows × 114 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('all_patients.xlsx')\n",
    "df = df[df.shape_VoxelVolume > 125]\n",
    "df.drop(df[df['labels'] == 3].index, inplace = True) ## dropping label of the call OTHERS\n",
    "df.drop(df[df['number_annotations'] > 1].index, inplace = True)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-20T12:22:09.186815Z",
     "end_time": "2023-06-20T12:22:11.857581Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "        Subjid  Tumor  Labels\n0       Oslo01      1       0\n4       Oslo02      4       1\n7       Oslo03      1       0\n11      Oslo04      1       0\n13      Oslo06      1       1\n...        ...    ...     ...\n2359  Stan_318      1       1\n2369  Stan_319      2       1\n2387  Stan_323      3       1\n2408  Stan_328      1       1\n2427  Stan_338      4       1\n\n[158 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Subjid</th>\n      <th>Tumor</th>\n      <th>Labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Oslo01</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Oslo02</td>\n      <td>4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Oslo03</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Oslo04</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Oslo06</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2359</th>\n      <td>Stan_318</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2369</th>\n      <td>Stan_319</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2387</th>\n      <td>Stan_323</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2408</th>\n      <td>Stan_328</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2427</th>\n      <td>Stan_338</td>\n      <td>4</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>158 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.DataFrame({'Subjid': df.iloc[:, 0], 'Tumor': df.iloc[:, 1], 'Labels': df.iloc[:, -1]})\n",
    "new_df = new_df.drop_duplicates(subset='Subjid', keep='first')\n",
    "#new_df.to_excel('labels_DL.xlsx', index = False)\n",
    "new_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-20T12:22:13.010982Z",
     "end_time": "2023-06-20T12:22:13.021748Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "stan = new_df['Subjid'][59:].to_list()\n",
    "oslo = new_df['Subjid'][:59].to_list()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-20T12:22:15.982891Z",
     "end_time": "2023-06-20T12:22:15.998772Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## we have created a new folder called deep learning with all the folder with all the folder per patient labels as 0, 1 or 2 with tumors greater than 125mm3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "outputs": [],
   "source": [
    "# Specify the path to the parent folder\n",
    "parent_folder = '/data/projects/TMOR/data/Deeplearning/'\n",
    "\n",
    "# Get a list of all subfolders within the parent folder\n",
    "subfolders = [f.path for f in os.scandir(parent_folder) if f.is_dir()]\n",
    "\n",
    "# Iterate over the subfolders and rename them\n",
    "\n",
    "for folder in subfolders:\n",
    "    folder_name = os.path.basename(folder)\n",
    "    new_folder_name = folder_name.replace(\"Subject\", \"Oslo\")\n",
    "    #new_folder_name = folder_name.replace(\"Mets\", \"Stan\")\n",
    "    new_folder_path = os.path.join(parent_folder, new_folder_name)\n",
    "    os.rename(folder, new_folder_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T17:47:59.313418Z",
     "end_time": "2023-06-19T17:47:59.341160Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "outputs": [],
   "source": [
    "#delete the other file for OSLO\n",
    "directory = '/data/projects/TMOR/data/Deeplearning/'\n",
    "\n",
    "# Get a list of all patient folders\n",
    "patient_folders = glob.glob(os.path.join(directory, 'Oslo*'))\n",
    "\n",
    "for patient_folder in patient_folders:\n",
    "    #id = patient_folder[-2:]\n",
    "    seg_folders = glob.glob(os.path.join(patient_folder, 'seg*'))\n",
    "    t1_gds = glob.glob(os.path.join(patient_folder, 't1_gd*'))\n",
    "    #print(seg_folders)\n",
    "    # Iterate over each seg folder\n",
    "    for seg_folder in seg_folders:\n",
    "        for t1_gd in t1_gds:\n",
    "            #print(seg_folder)\n",
    "            image_files = glob.glob(os.path.join(patient_folder, '*'))\n",
    "            #print(image_files)\n",
    "            # Delete all the image files except for 'seg_1'\n",
    "            for file in image_files:\n",
    "            # Check if the file is in the segmentation folder or T1-Gd folder\n",
    "                if file not in [seg_folder, t1_gd]:\n",
    "                    os.remove(file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T17:48:07.240665Z",
     "end_time": "2023-06-19T17:48:07.380290Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "outputs": [],
   "source": [
    "patient_folders = glob.glob(os.path.join(directory, 'Oslo*'))\n",
    "import shutil\n",
    "for patient_folder in patient_folders:\n",
    "    id = patient_folder[-6:]\n",
    "    if id not in oslo:\n",
    "        shutil.rmtree(patient_folder)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T17:48:30.041220Z",
     "end_time": "2023-06-19T17:48:30.067252Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "directory = '/data/projects/TMOR/data/Deeplearning/'\n",
    "\n",
    "patient_folders = glob.glob(os.path.join(directory, 'Oslo*'))\n",
    "\n",
    "\n",
    "# Iterate over patient folders\n",
    "for patient_folder in patient_folders:\n",
    "    #patient_path = os.path.join(directory, patient_folder)\n",
    "    seg_folders = glob.glob(os.path.join(patient_folder, 'seg*'))\n",
    "    #print(patient_path)\n",
    "    # Find segmentation files in the patient folder\n",
    "    #seg_files = [f for f in os.listdir(patient_path) if f.startswith('seg_')]\n",
    "    #seg_folders = glob.glob(os.path.join(patient_path, 'seg*'))\n",
    "    #print(patient_folder)\n",
    "    # Rename segmentation files\n",
    "    for seg_folder in seg_folders:\n",
    "        seg_old_path = seg_folder\n",
    "        seg_new_path = os.path.join(patient_folder, 'seg.nii.gz')\n",
    "        os.rename(seg_old_path, seg_new_path)\n",
    "        #print(seg_new_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T17:48:31.682668Z",
     "end_time": "2023-06-19T17:48:31.726335Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## STANDFORD DATA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "outputs": [],
   "source": [
    "# Specify the path to the parent folder\n",
    "parent_folder = '/data/projects/TMOR/data/Deeplearning/'\n",
    "\n",
    "# Get a list of all subfolders within the parent folder\n",
    "subfolders = [f.path for f in os.scandir(parent_folder) if f.is_dir()]\n",
    "\n",
    "# Iterate over the subfolders and rename them\n",
    "\n",
    "for folder in subfolders:\n",
    "    folder_name = os.path.basename(folder)\n",
    "    #new_folder_name = folder_name.replace(\"Subject\", \"Oslo\")\n",
    "    new_folder_name = folder_name.replace(\"Mets\", \"Stan\")\n",
    "    new_folder_path = os.path.join(parent_folder, new_folder_name)\n",
    "    os.rename(folder, new_folder_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T17:51:09.426017Z",
     "end_time": "2023-06-19T17:51:09.465564Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "#delete the other file for OSLO\n",
    "directory = '/data/projects/TMOR/data/Deeplearning/'\n",
    "\n",
    "# Get a list of all patient folders\n",
    "patient_folders = glob.glob(os.path.join(directory, 'Stan*'))\n",
    "\n",
    "for patient_folder in patient_folders:\n",
    "    #id = patient_folder[-2:]\n",
    "    seg_folders = glob.glob(os.path.join(patient_folder, 'seg*'))\n",
    "    t1_gds = glob.glob(os.path.join(patient_folder, 't1_gd*'))\n",
    "    #print(seg_folders)\n",
    "    # Iterate over each seg folder\n",
    "    for seg_folder in seg_folders:\n",
    "        for t1_gd in t1_gds:\n",
    "            #print(seg_folder)\n",
    "            image_files = glob.glob(os.path.join(patient_folder, '*'))\n",
    "            #print(image_files)\n",
    "            # Delete all the image files except for 'seg_1'\n",
    "            for file in image_files:\n",
    "                # Check if the file is in the segmentation folder or T1-Gd folder\n",
    "                if file not in [seg_folder, t1_gd]:\n",
    "                    os.remove(file)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T17:51:13.837773Z",
     "end_time": "2023-06-19T17:51:14.245613Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "outputs": [],
   "source": [
    "#Remove all the files that are not within stan\n",
    "import shutil\n",
    "for patient_folder in patient_folders:\n",
    "    id = patient_folder[-8:]\n",
    "    if id not in stan:\n",
    "        shutil.rmtree(patient_folder)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T17:51:18.497052Z",
     "end_time": "2023-06-19T17:51:18.561806Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "outputs": [],
   "source": [
    "\n",
    "class BrainSegmentationDataset(Dataset):\n",
    "    def __init__(self, img_dir, labels_file, transform=None, target_transform=None):\n",
    "        self.img_dir= img_dir\n",
    "        self.img_labels = pd.read_excel(labels_file)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        #self.data_files = [os.path.basename(file) for file in self.img]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #data_file = self.data_files[index]\n",
    "        seg_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0], 'seg.nii.gz')\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0], 't1_gd.nii.gz')\n",
    "        label = self.img_labels.iloc[idx, 2]\n",
    "\n",
    "        # Load data and mask using nibabel\n",
    "        data = nib.load(seg_path).get_fdata()\n",
    "        mask = nib.load(img_path).get_fdata()\n",
    "\n",
    "        # Load label for the corresponding data file\n",
    "        #label = self.labels[data_file]\n",
    "\n",
    "        # Apply transformations (if any)\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        # Normalize data and convert to torch tensors\n",
    "        data = torch.from_numpy(data).unsqueeze(0).float()\n",
    "        mask = torch.from_numpy(mask).unsqueeze(0).float()\n",
    "        label = torch.tensor(label).long()\n",
    "\n",
    "        return data, mask, label\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T17:52:55.448578Z",
     "end_time": "2023-06-19T17:52:55.490977Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "outputs": [],
   "source": [
    "img_dir = '/data/projects/TMOR/data/Deeplearning/'\n",
    "labels_file = 'labels_DL.xlsx'\n",
    "transform = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])\n",
    "\n",
    "dataset = BrainSegmentationDataset(img_dir,labels_file, transform=transform)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T17:52:58.341917Z",
     "end_time": "2023-06-19T17:52:58.381549Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 438\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unexpected type <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[335], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Print information for a specific sample\u001B[39;00m\n\u001B[1;32m      4\u001B[0m index \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m  \u001B[38;5;66;03m# Index of the sample\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m sample \u001B[38;5;241m=\u001B[39m \u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m      6\u001B[0m images, label \u001B[38;5;241m=\u001B[39m sample\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSample images:\u001B[39m\u001B[38;5;124m\"\u001B[39m, images)\n",
      "Cell \u001B[0;32mIn[333], line 28\u001B[0m, in \u001B[0;36mBrainSegmentationDataset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# Load label for the corresponding data file\u001B[39;00m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m#label = self.labels[data_file]\u001B[39;00m\n\u001B[1;32m     25\u001B[0m \n\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m# Apply transformations (if any)\u001B[39;00m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 28\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     29\u001B[0m     mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(mask)\n\u001B[1;32m     31\u001B[0m \u001B[38;5;66;03m# Normalize data and convert to torch tensors\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/site-packages/torchvision/transforms/transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[1;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[0;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/site-packages/torchvision/transforms/transforms.py:361\u001B[0m, in \u001B[0;36mResize.forward\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m    353\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[1;32m    354\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    355\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m    356\u001B[0m \u001B[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    359\u001B[0m \u001B[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001B[39;00m\n\u001B[1;32m    360\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 361\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minterpolation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mantialias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/site-packages/torchvision/transforms/functional.py:476\u001B[0m, in \u001B[0;36mresize\u001B[0;34m(img, size, interpolation, max_size, antialias)\u001B[0m\n\u001B[1;32m    470\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m max_size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(size) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    471\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    472\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_size should only be passed if size specifies the length of the smaller edge, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    473\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mi.e. size should be an int or a sequence of length 1 in torchscript mode.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    474\u001B[0m         )\n\u001B[0;32m--> 476\u001B[0m _, image_height, image_width \u001B[38;5;241m=\u001B[39m \u001B[43mget_dimensions\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(size, \u001B[38;5;28mint\u001B[39m):\n\u001B[1;32m    478\u001B[0m     size \u001B[38;5;241m=\u001B[39m [size]\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/site-packages/torchvision/transforms/functional.py:78\u001B[0m, in \u001B[0;36mget_dimensions\u001B[0;34m(img)\u001B[0m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(img, torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[1;32m     76\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F_t\u001B[38;5;241m.\u001B[39mget_dimensions(img)\n\u001B[0;32m---> 78\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF_pil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_dimensions\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/site-packages/torchvision/transforms/_functional_pil.py:31\u001B[0m, in \u001B[0;36mget_dimensions\u001B[0;34m(img)\u001B[0m\n\u001B[1;32m     29\u001B[0m     width, height \u001B[38;5;241m=\u001B[39m img\u001B[38;5;241m.\u001B[39msize\n\u001B[1;32m     30\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [channels, height, width]\n\u001B[0;32m---> 31\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnexpected type \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(img)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mTypeError\u001B[0m: Unexpected type <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "print(\"Dataset length:\", len(dataset))\n",
    "\n",
    "# Print information for a specific sample\n",
    "index = 0  # Index of the sample\n",
    "sample = dataset[index]\n",
    "images, label = sample\n",
    "print(\"Sample images:\", images)\n",
    "print(\"Label:\", label)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-16T14:08:27.339358Z",
     "end_time": "2023-06-16T14:08:27.348645Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 438\n",
      "Data shape: torch.Size([1, 127, 172, 136])\n",
      "Mask shape: torch.Size([1, 127, 172, 136])\n",
      "Label: tensor(1)\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the dataset\n",
    "dataset = BrainSegmentationDataset(img_dir=img_dir, labels_file=labels_file)\n",
    "\n",
    "print(\"Dataset length:\", len(dataset))\n",
    "\n",
    "# Access an item from the dataset\n",
    "sample_data, sample_mask, sample_label = dataset[118]\n",
    "\n",
    "# Verify the shapes of the data, mask, and label\n",
    "print('Data shape:', sample_data.shape)\n",
    "print('Mask shape:', sample_mask.shape)\n",
    "print('Label:', sample_label)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T17:53:24.447877Z",
     "end_time": "2023-06-19T17:53:24.555741Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## lets try divide by train an test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class BrainSegmentationDataset(Dataset):\n",
    "    def __init__(self, img_dir, labels_file, transform=None, target_transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_labels = pd.read_excel(labels_file)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = self.split_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.transform is None:\n",
    "            seg_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0], 'seg.nii.gz')\n",
    "            img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0], 't1_gd.nii.gz')\n",
    "            label = self.y_train[idx]\n",
    "            data = nib.load(seg_path).get_fdata()\n",
    "            mask = nib.load(img_path).get_fdata()\n",
    "\n",
    "        else:\n",
    "            seg_path = os.path.join(self.img_dinr, self.img_labels.iloc[idx, 0], 'seg.nii.gz')\n",
    "            img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0], 't1_gd.nii.gz')\n",
    "            label = self.y_train[idx]\n",
    "\n",
    "            data = nib.load(seg_path).get_fdata()\n",
    "            mask = nib.load(img_path).get_fdata()\n",
    "\n",
    "        data = torch.from_numpy(data).unsqueeze(0)\n",
    "        mask = torch.from_numpy(mask).unsqueeze(0)\n",
    "        label = torch.tensor(label).long()\n",
    "\n",
    "        return data, mask, label\n",
    "\n",
    "    def split_data(self):\n",
    "        X = self.img_labels.iloc[:, 0].values\n",
    "        y = self.img_labels.iloc[:, 2].values\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y ,random_state=0)\n",
    "\n",
    "        X_train = self.process_images(X_train)\n",
    "        X_test = self.process_images(X_test)\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def process_images(self, image_names):\n",
    "        image_tensors = []\n",
    "\n",
    "        for image_name in image_names:\n",
    "            if self.transform is None:\n",
    "                seg_path = os.path.join(self.img_dir, self.img_labels.iloc[image_name, 0], 'seg.nii.gz')\n",
    "                img_path = os.path.join(self.img_dir, self.img_labels.iloc[image_name, 0], 't1_gd.nii.gz')\n",
    "\n",
    "                data = nib.load(seg_path).get_fdata()\n",
    "                mask = nib.load(img_path).get_fdata()\n",
    "\n",
    "            else:\n",
    "                data = self.transform(data)\n",
    "                mask = self.transform(mask)\n",
    "\n",
    "            data_tensor = torch.from_numpy(data).unsqueeze(0)\n",
    "            mask_tensor = torch.from_numpy(mask).unsqueeze(0)\n",
    "\n",
    "            image_tensors.append((data_tensor, mask_tensor))\n",
    "\n",
    "        return image_tensors\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T19:39:59.873594Z",
     "end_time": "2023-06-19T19:39:59.913666Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Location based indexing can only have [integer, integer slice (START point is INCLUDED, END point is EXCLUDED), listlike of integers, boolean array] types",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/site-packages/pandas/core/indexing.py:904\u001B[0m, in \u001B[0;36m_LocationIndexer._validate_tuple_indexer\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    903\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 904\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_key\u001B[49m\u001B[43m(\u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    905\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/site-packages/pandas/core/indexing.py:1518\u001B[0m, in \u001B[0;36m_iLocIndexer._validate_key\u001B[0;34m(self, key, axis)\u001B[0m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCan only index by location with a [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_valid_types\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: Can only index by location with a [integer, integer slice (START point is INCLUDED, END point is EXCLUDED), listlike of integers, boolean array]",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m labels_file \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels_DL.xlsx\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m#transform = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mBrainSegmentationDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43mlabels_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransform\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[4], line 15\u001B[0m, in \u001B[0;36mBrainSegmentationDataset.__init__\u001B[0;34m(self, img_dir, labels_file, transform, target_transform)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;241m=\u001B[39m transform\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform \u001B[38;5;241m=\u001B[39m target_transform\n\u001B[0;32m---> 15\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mX_train, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mX_test, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39my_train, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39my_test \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[4], line 48\u001B[0m, in \u001B[0;36mBrainSegmentationDataset.split_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     44\u001B[0m y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimg_labels\u001B[38;5;241m.\u001B[39miloc[:, \u001B[38;5;241m2\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues\n\u001B[1;32m     46\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m train_test_split(X, y, test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m, stratify\u001B[38;5;241m=\u001B[39my ,random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m---> 48\u001B[0m X_train \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess_images\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m X_test \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_images(X_test)\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m X_train, X_test, y_train, y_test\n",
      "Cell \u001B[0;32mIn[4], line 58\u001B[0m, in \u001B[0;36mBrainSegmentationDataset.process_images\u001B[0;34m(self, image_names)\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m image_name \u001B[38;5;129;01min\u001B[39;00m image_names:\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m         seg_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimg_dir, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimg_labels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miloc\u001B[49m\u001B[43m[\u001B[49m\u001B[43mimage_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mseg.nii.gz\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     59\u001B[0m         img_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimg_dir, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimg_labels\u001B[38;5;241m.\u001B[39miloc[image_name, \u001B[38;5;241m0\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt1_gd.nii.gz\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     61\u001B[0m         data \u001B[38;5;241m=\u001B[39m nib\u001B[38;5;241m.\u001B[39mload(seg_path)\u001B[38;5;241m.\u001B[39mget_fdata()\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/site-packages/pandas/core/indexing.py:1097\u001B[0m, in \u001B[0;36m_LocationIndexer.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   1095\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_is_scalar_access(key):\n\u001B[1;32m   1096\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj\u001B[38;5;241m.\u001B[39m_get_value(\u001B[38;5;241m*\u001B[39mkey, takeable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_takeable)\n\u001B[0;32m-> 1097\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem_tuple\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1098\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1099\u001B[0m     \u001B[38;5;66;03m# we by definition only have the 0th axis\u001B[39;00m\n\u001B[1;32m   1100\u001B[0m     axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maxis \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/site-packages/pandas/core/indexing.py:1594\u001B[0m, in \u001B[0;36m_iLocIndexer._getitem_tuple\u001B[0;34m(self, tup)\u001B[0m\n\u001B[1;32m   1593\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_getitem_tuple\u001B[39m(\u001B[38;5;28mself\u001B[39m, tup: \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m-> 1594\u001B[0m     tup \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_tuple_indexer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtup\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1595\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m suppress(IndexingError):\n\u001B[1;32m   1596\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_lowerdim(tup)\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/site-packages/pandas/core/indexing.py:906\u001B[0m, in \u001B[0;36m_LocationIndexer._validate_tuple_indexer\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    904\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_key(k, i)\n\u001B[1;32m    905\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m--> 906\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    907\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLocation based indexing can only have \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    908\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_valid_types\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] types\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    909\u001B[0m         ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m    910\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m key\n",
      "\u001B[0;31mValueError\u001B[0m: Location based indexing can only have [integer, integer slice (START point is INCLUDED, END point is EXCLUDED), listlike of integers, boolean array] types"
     ]
    }
   ],
   "source": [
    "img_dir = '/data/projects/TMOR/data/Deeplearning/'\n",
    "labels_file = 'labels_DL.xlsx'\n",
    "#transform = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])\n",
    "\n",
    "dataset = BrainSegmentationDataset(img_dir,labels_file, transform=None)\n",
    "#data_loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-16T19:00:09.877380Z",
     "end_time": "2023-06-16T19:00:43.786875Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = dataset.X_train, dataset.X_test, dataset.y_train, dataset.y_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-16T19:00:50.846636Z",
     "end_time": "2023-06-16T19:00:50.891835Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-113.0706, -112.6847, -109.6506,  ...,  111.3742,  112.9775,\n         114.3297], dtype=torch.float64)"
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = dataset.X_train, dataset.X_test, dataset.y_train, dataset.y_test\n",
    "X_train[0][1].sum(axis=0).sum(axis=1).unique()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-16T19:00:53.070160Z",
     "end_time": "2023-06-16T19:00:53.092976Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 2, 2, 1, 2, 1, 1, 1,\n       0, 1, 1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 0, 2, 0, 1, 0, 1, 1, 1, 1,\n       2, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2,\n       1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 2, 1,\n       1, 2, 0, 0, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n       2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1,\n       1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1,\n       1, 0, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1,\n       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2,\n       1, 1, 1, 1, 2, 0, 2, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1,\n       1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 0, 1, 1, 2, 1, 2, 1, 1, 2, 1,\n       1, 1, 1, 1, 1, 1, 1, 0, 1, 2, 0, 2, 2, 1, 1, 1, 1, 1, 1, 0, 2, 1,\n       1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 1,\n       1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 0, 2, 1, 1, 1, 0, 1, 2, 2, 1, 0, 2, 1, 2, 1, 1])"
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-16T18:18:11.837019Z",
     "end_time": "2023-06-16T18:18:11.847080Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class BrainSegmentationDataset(Dataset):\n",
    "    def __init__(self, img_dir, labels_file, transform=None, target_transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_labels = pd.read_excel(labels_file)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = self.split_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seg_path = os.path.join(self.img_dir, self.X_train.iloc[idx, 0], 'seg.nii.gz')\n",
    "        img_path = os.path.join(self.img_dir, self.X_train.iloc[idx, 0], 't1_gd.nii.gz')\n",
    "        label = self.y_train[idx]\n",
    "\n",
    "        data = self.preprocess_image(seg_path)\n",
    "        mask = self.preprocess_image(img_path)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        data = torch.from_numpy(data).unsqueeze(0).float()\n",
    "        mask = torch.from_numpy(mask).unsqueeze(0).float()\n",
    "        label = torch.tensor(label).long()\n",
    "\n",
    "        return data, mask, label\n",
    "\n",
    "    def preprocess_image(self, image_path):\n",
    "        image = nib.load(image_path).get_fdata()\n",
    "        # Perform preprocessing steps here (e.g., normalization, resizing, cropping, etc.)\n",
    "        # Preprocess image and return the processed image\n",
    "\n",
    "        return processed_image\n",
    "\n",
    "    def split_data(self):\n",
    "        X = self.img_labels.iloc[:, 0].values\n",
    "        y = self.img_labels.iloc[:, 2].values\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        X_train = pd.DataFrame(X_train, columns=['Filename'])\n",
    "        X_test = pd.DataFrame(X_test, columns=['Filename'])\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unexpected type <class 'nibabel.nifti1.Nifti1Image'>",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 97\u001B[0m\n\u001B[1;32m     94\u001B[0m labels_file \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels_DL.xlsx\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     95\u001B[0m transform \u001B[38;5;241m=\u001B[39m transforms\u001B[38;5;241m.\u001B[39mCompose([transforms\u001B[38;5;241m.\u001B[39mResize((\u001B[38;5;241m256\u001B[39m, \u001B[38;5;241m256\u001B[39m)), transforms\u001B[38;5;241m.\u001B[39mToTensor()])\n\u001B[0;32m---> 97\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mBrainSegmentationDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransform\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     98\u001B[0m data_loader \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataLoader(dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m16\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "Cell \u001B[0;32mIn[18], line 8\u001B[0m, in \u001B[0;36mBrainSegmentationDataset.__init__\u001B[0;34m(self, img_dir, labels_file, transform, target_transform)\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;241m=\u001B[39m transform\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform \u001B[38;5;241m=\u001B[39m target_transform\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mX_train, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mX_test, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39my_train, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39my_test \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[18], line 51\u001B[0m, in \u001B[0;36mBrainSegmentationDataset.split_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     47\u001B[0m y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimg_labels\u001B[38;5;241m.\u001B[39miloc[:, \u001B[38;5;241m2\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues\n\u001B[1;32m     49\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m train_test_split(X, y, test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m, stratify\u001B[38;5;241m=\u001B[39my, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m---> 51\u001B[0m X_train \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess_images\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     52\u001B[0m X_test \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_images(X_test)\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m X_train, X_test, y_train, y_test\n",
      "Cell \u001B[0;32mIn[18], line 76\u001B[0m, in \u001B[0;36mBrainSegmentationDataset.process_images\u001B[0;34m(self, image_names)\u001B[0m\n\u001B[1;32m     74\u001B[0m img_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimg_dir, image_name, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt1_gd.nii.gz\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     75\u001B[0m data_1 \u001B[38;5;241m=\u001B[39m nib\u001B[38;5;241m.\u001B[39mload(img_path)\n\u001B[0;32m---> 76\u001B[0m data_2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_1\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     77\u001B[0m data_3 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconvert_to_grayscale(data_2)\n\u001B[1;32m     78\u001B[0m data \u001B[38;5;241m=\u001B[39m data_3\u001B[38;5;241m.\u001B[39mget_fdata()\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/site-packages/torchvision/transforms/transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[1;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[0;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/site-packages/torchvision/transforms/transforms.py:361\u001B[0m, in \u001B[0;36mResize.forward\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m    353\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[1;32m    354\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    355\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m    356\u001B[0m \u001B[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    359\u001B[0m \u001B[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001B[39;00m\n\u001B[1;32m    360\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 361\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minterpolation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mantialias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/site-packages/torchvision/transforms/functional.py:476\u001B[0m, in \u001B[0;36mresize\u001B[0;34m(img, size, interpolation, max_size, antialias)\u001B[0m\n\u001B[1;32m    470\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m max_size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(size) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    471\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    472\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_size should only be passed if size specifies the length of the smaller edge, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    473\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mi.e. size should be an int or a sequence of length 1 in torchscript mode.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    474\u001B[0m         )\n\u001B[0;32m--> 476\u001B[0m _, image_height, image_width \u001B[38;5;241m=\u001B[39m \u001B[43mget_dimensions\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(size, \u001B[38;5;28mint\u001B[39m):\n\u001B[1;32m    478\u001B[0m     size \u001B[38;5;241m=\u001B[39m [size]\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/site-packages/torchvision/transforms/functional.py:78\u001B[0m, in \u001B[0;36mget_dimensions\u001B[0;34m(img)\u001B[0m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(img, torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[1;32m     76\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F_t\u001B[38;5;241m.\u001B[39mget_dimensions(img)\n\u001B[0;32m---> 78\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF_pil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_dimensions\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/site-packages/torchvision/transforms/_functional_pil.py:31\u001B[0m, in \u001B[0;36mget_dimensions\u001B[0;34m(img)\u001B[0m\n\u001B[1;32m     29\u001B[0m     width, height \u001B[38;5;241m=\u001B[39m img\u001B[38;5;241m.\u001B[39msize\n\u001B[1;32m     30\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [channels, height, width]\n\u001B[0;32m---> 31\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnexpected type \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(img)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mTypeError\u001B[0m: Unexpected type <class 'nibabel.nifti1.Nifti1Image'>"
     ]
    }
   ],
   "source": [
    "class BrainSegmentationDataset(Dataset):\n",
    "    def __init__(self, img_dir, labels_file, transform=None, target_transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_labels = pd.read_excel(labels_file)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = self.split_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.transform is None:\n",
    "            # Load the mask directly\n",
    "            seg_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0], 'seg.nii.gz')\n",
    "            img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0], 't1_gd.nii.gz')\n",
    "            data_1 = nib.load(img_path)\n",
    "            data_2 = self.convert_to_grayscale(data_1)\n",
    "            data = data_2.get_fdata()\n",
    "            mask_1 = nib.load(seg_path)\n",
    "            mask_2 = self.convert_to_grayscale(mask_1)\n",
    "            mask = mask_2.get_fdata()\n",
    "            label = self.y_train[idx]\n",
    "        else:\n",
    "            # Apply the transformation to the mask\n",
    "            seg_path = os.path.join(self.img_dinr, self.img_labels.iloc[idx, 0], 'seg.nii.gz')\n",
    "            img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0], 't1_gd.nii.gz')\n",
    "            data_1 = nib.load(img_path)\n",
    "            data_2 = self.transform(data_1)\n",
    "            data_3 = self.convert_to_grayscale(data_2)\n",
    "            data = data_3.get_fdata()\n",
    "            mask_1 = nib.load(seg_path)\n",
    "            mask_2 = self.transform(mask_1)\n",
    "            mask_3 = self.convert_to_grayscale(mask_2)\n",
    "            mask = mask_3.get_fdata()\n",
    "            label = self.y_train[idx]\n",
    "\n",
    "\n",
    "        mask = torch.from_numpy(mask).unsqueeze(0)\n",
    "        label = torch.tensor(label).long()\n",
    "\n",
    "        return mask, data,label\n",
    "\n",
    "    def split_data(self):\n",
    "        X = self.img_labels.iloc[:, 0].values\n",
    "        y = self.img_labels.iloc[:, 2].values\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)\n",
    "\n",
    "        X_train = self.process_images(X_train)\n",
    "        X_test = self.process_images(X_test)\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def process_images(self, image_names):\n",
    "        image_tensors = []\n",
    "        #pene = list(image_names[:, 0])\n",
    "        for image_name in image_names:\n",
    "            if self.transform is None:\n",
    "                # Load the mask directly\n",
    "                seg_path = os.path.join(self.img_dir, image_name, 'seg.nii.gz')\n",
    "                img_path = os.path.join(self.img_dir, image_name, 't1_gd.nii.gz')\n",
    "                data_1 = nib.load(img_path)\n",
    "                data_2 = self.convert_to_grayscale(data_1)\n",
    "                data = data_2.get_fdata()\n",
    "                mask_1 = nib.load(seg_path)\n",
    "                mask_2 = self.convert_to_grayscale(mask_1)\n",
    "                mask = mask_2.get_fdata()\n",
    "\n",
    "            else:\n",
    "                # Apply the transformation to the mask\n",
    "                seg_path = os.path.join(self.img_dir, image_name, 'seg.nii.gz')\n",
    "                img_path = os.path.join(self.img_dir, image_name, 't1_gd.nii.gz')\n",
    "                data_1 = nib.load(img_path)\n",
    "                data_2 = self.transform(data_1)\n",
    "                data_3 = self.convert_to_grayscale(data_2)\n",
    "                data = data_3.get_fdata()\n",
    "                mask_1 = nib.load(seg_path)\n",
    "                mask_2 = self.transform(mask_1)\n",
    "                mask_3 = self.convert_to_grayscale(mask_2)\n",
    "                mask = mask_3.get_fdata()\n",
    "\n",
    "\n",
    "            data_tensor = torch.from_numpy(data).unsqueeze(0)\n",
    "            mask_tensor = torch.from_numpy(mask).unsqueeze(0)\n",
    "\n",
    "            image_tensors.append((data_tensor, mask_tensor))\n",
    "\n",
    "        return image_tensors\n",
    "\n",
    "\n",
    "img_dir = '/data/projects/TMOR/data/Deeplearning/'\n",
    "labels_file = 'labels_DL.xlsx'\n",
    "transform = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])\n",
    "\n",
    "dataset = BrainSegmentationDataset(img_dir, labels_file, transform=transform)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T19:55:44.174638Z",
     "end_time": "2023-06-19T19:55:44.186410Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1344\n",
      "1344\n",
      "0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[34], line 103\u001B[0m\n\u001B[1;32m    100\u001B[0m labels_file \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels_DL.xlsx\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    101\u001B[0m transform \u001B[38;5;241m=\u001B[39m transforms\u001B[38;5;241m.\u001B[39mCompose([transforms\u001B[38;5;241m.\u001B[39mResize((\u001B[38;5;241m256\u001B[39m, \u001B[38;5;241m256\u001B[39m)), transforms\u001B[38;5;241m.\u001B[39mToTensor()])\n\u001B[0;32m--> 103\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mBrainSegmentationDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransform\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    104\u001B[0m data_loader \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataLoader(dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m16\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "Cell \u001B[0;32mIn[34], line 21\u001B[0m, in \u001B[0;36mBrainSegmentationDataset.__init__\u001B[0;34m(self, img_dir, labels_file, transform, target_transform)\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;241m=\u001B[39m transform\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform \u001B[38;5;241m=\u001B[39m target_transform\n\u001B[0;32m---> 21\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mX_train, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mX_test, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39my_train, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39my_test \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[34], line 61\u001B[0m, in \u001B[0;36mBrainSegmentationDataset.split_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     57\u001B[0m y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimg_labels\u001B[38;5;241m.\u001B[39miloc[:, \u001B[38;5;241m2\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues\n\u001B[1;32m     59\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m train_test_split(X, y, test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m, stratify\u001B[38;5;241m=\u001B[39my, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m---> 61\u001B[0m X_train \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess_images\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     62\u001B[0m X_test \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_images(X_test)\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m X_train, X_test, y_train, y_test\n",
      "Cell \u001B[0;32mIn[34], line 89\u001B[0m, in \u001B[0;36mBrainSegmentationDataset.process_images\u001B[0;34m(self, image_names)\u001B[0m\n\u001B[1;32m     86\u001B[0m end \u001B[38;5;241m=\u001B[39m start \u001B[38;5;241m+\u001B[39m max_tumor_size\n\u001B[1;32m     88\u001B[0m \u001B[38;5;66;03m# Crop image and mask tensors\u001B[39;00m\n\u001B[0;32m---> 89\u001B[0m data \u001B[38;5;241m=\u001B[39m data[\u001B[43mstart\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m:end[\u001B[38;5;241m0\u001B[39m], start[\u001B[38;5;241m1\u001B[39m]:end[\u001B[38;5;241m1\u001B[39m], start[\u001B[38;5;241m2\u001B[39m]:end[\u001B[38;5;241m2\u001B[39m]]\n\u001B[1;32m     90\u001B[0m mask \u001B[38;5;241m=\u001B[39m mask[start[\u001B[38;5;241m0\u001B[39m]:end[\u001B[38;5;241m0\u001B[39m], start[\u001B[38;5;241m1\u001B[39m]:end[\u001B[38;5;241m1\u001B[39m], start[\u001B[38;5;241m2\u001B[39m]:end[\u001B[38;5;241m2\u001B[39m]]\n\u001B[1;32m     92\u001B[0m data_tensor \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(data)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n",
      "\u001B[0;31mIndexError\u001B[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "def crop_to_largest_tumor(tensor, crop_size):\n",
    "    # Get tumor dimensions\n",
    "    tumor_size = np.sum(tensor > 0, axis=(1, 2, 3))\n",
    "    max_tumor_size = np.max(tumor_size)\n",
    "\n",
    "    # Calculate cropping indices\n",
    "    start = (tumor_size - max_tumor_size) // 2\n",
    "    end = start + max_tumor_size\n",
    "\n",
    "    # Perform cropping\n",
    "    cropped_tensor = tensor[:, start:end, start:end, start:end]\n",
    "\n",
    "    return cropped_tensor\n",
    "class BrainSegmentationDataset(Dataset):\n",
    "    def __init__(self, img_dir, labels_file, transform=None, target_transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_labels = pd.read_excel(labels_file)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = self.split_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the mask directly\n",
    "        seg_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0], 'seg.nii.gz')\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0], 't1_gd.nii.gz')\n",
    "        data_1 = nib.load(img_path)\n",
    "        data = data_1.get_fdata()\n",
    "        mask_1 = nib.load(seg_path)\n",
    "        mask = mask_1.get_fdata()\n",
    "        label = self.y_train[idx]\n",
    "\n",
    "        data = torch.from_numpy(data).unsqueeze(0)\n",
    "        mask = torch.from_numpy(mask).unsqueeze(0)\n",
    "        label = torch.tensor(label).long()\n",
    "\n",
    "        return mask, data, label\n",
    "\n",
    "    def split_data(self):\n",
    "        X = self.img_labels.iloc[:, 0].values\n",
    "        y = self.img_labels.iloc[:, 2].values\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)\n",
    "\n",
    "        X_train = self.process_images(X_train)\n",
    "        X_test = self.process_images(X_test)\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def process_images(self, image_names):\n",
    "        image_tensors = []\n",
    "\n",
    "        for image_name in image_names:\n",
    "            # Load the mask directly\n",
    "            seg_path = os.path.join(self.img_dir, image_name, 'seg.nii.gz')\n",
    "            img_path = os.path.join(self.img_dir, image_name, 't1_gd.nii.gz')\n",
    "            data_1 = nib.load(img_path)\n",
    "            data = data_1.get_fdata()\n",
    "            mask_1 = nib.load(seg_path)\n",
    "            mask = mask_1.get_fdata()\n",
    "\n",
    "            data_tensor = torch.from_numpy(data).unsqueeze(0)\n",
    "            mask_tensor = torch.from_numpy(mask).unsqueeze(0)\n",
    "\n",
    "            image_tensors.append((data_tensor, mask_tensor))\n",
    "\n",
    "        return image_tensors\n",
    "\n",
    "            tumor_size = np.sum(mask > 0, axis=(0, 1, 2))\n",
    "            print(tumor_size)\n",
    "            max_tumor_size = np.max(tumor_size)\n",
    "            print(max_tumor_size)\n",
    "\n",
    "            # Calculate cropping indices\n",
    "            start = (tumor_size - max_tumor_size) // 2\n",
    "            print(start)\n",
    "            end = start + max_tumor_size\n",
    "\n",
    "            # Crop image and mask tensors\n",
    "            data = data[start[0]:end[0], start[1]:end[1], start[2]:end[2]]\n",
    "            mask = mask[start[0]:end[0], start[1]:end[1], start[2]:end[2]]\n",
    "\n",
    "            data_tensor = torch.from_numpy(data).unsqueeze(0)\n",
    "            mask_tensor = torch.from_numpy(mask).unsqueeze(0)\n",
    "\n",
    "            image_tensors.append((data_tensor, mask_tensor))\n",
    "\n",
    "        return image_tensors\n",
    "\n",
    "img_dir = '/data/projects/TMOR/data/Deeplearning/'\n",
    "labels_file = 'labels_DL.xlsx'\n",
    "transform = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])\n",
    "\n",
    "dataset = BrainSegmentationDataset(img_dir, labels_file, transform=transform)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T20:46:34.373136Z",
     "end_time": "2023-06-19T20:47:15.555267Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess_nifti_scan(file_path, threshold_value=0.5):\n",
    "    # Load the NIfTI scan using nibabel\n",
    "    nifti_img = nib.load(file_path)\n",
    "\n",
    "    # Get the scan data and header\n",
    "    scan_data = np.array(nifti_img.dataobj)\n",
    "    scan_header = nifti_img.header\n",
    "\n",
    "    # Apply preprocessing steps\n",
    "    # Replace this section with your specific preprocessing operations\n",
    "\n",
    "    # Normalize the scan data\n",
    "    scan_data = (scan_data - np.mean(scan_data)) / np.std(scan_data)\n",
    "\n",
    "    # Convert the scan data to the desired format\n",
    "    # (e.g., float32, 0-1 range, etc.)\n",
    "    scan_data = scan_data.astype(np.float32)\n",
    "    scan_data = np.clip(scan_data, 0.0, 1.0)\n",
    "\n",
    "    # Get the tumor bounding boxes\n",
    "    tumor_bounding_boxes = get_tumor_bounding_boxes(scan_data, threshold_value)\n",
    "\n",
    "    # Generate the labels for each bounding box\n",
    "    labels = [1] * len(tumor_bounding_boxes)\n",
    "\n",
    "    # Return the preprocessed data and labels\n",
    "    return scan_data, labels\n",
    "\n",
    "\n",
    "def get_tumor_bounding_boxes(scan_data, threshold_value=0.5):\n",
    "    # Threshold the scan data to separate tumors from background\n",
    "    thresholded_scan = np.where(scan_data >= threshold_value, 1, 0)\n",
    "\n",
    "    # Perform connected component analysis to identify separate tumor regions\n",
    "    num_labels, labeled_scan = cv2.connectedComponents(thresholded_scan.astype(np.uint8))\n",
    "\n",
    "    # Iterate through each label (excluding the background label) and extract bounding boxes\n",
    "    tumor_bounding_boxes = []\n",
    "    for label in range(1, num_labels):\n",
    "        # Extract the region corresponding to the current label\n",
    "        region_mask = np.where(labeled_scan == label, 1, 0)\n",
    "\n",
    "        # Find the bounding box coordinates using contour extraction\n",
    "        contours, _ = cv2.findContours(region_mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        x, y, w, h = cv2.boundingRect(contours[0])\n",
    "\n",
    "        # Append the bounding box coordinates to the list\n",
    "        tumor_bounding_boxes.append((x, y, w, h))\n",
    "\n",
    "    return tumor_bounding_boxes\n",
    "\n",
    "\n",
    "class BrainSegmentationDataset(Dataset):\n",
    "    def __init__(self, img_dir, labels_file, transform=None, target_transform=None, threshold_value=0.5):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_labels = pd.read_excel(labels_file)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.threshold_value = threshold_value\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = self.split_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the mask directly\n",
    "        seg_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0], 'seg.nii.gz')\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0], 't1_gd.nii.gz')\n",
    "        data_1 = nib.load(img_path)\n",
    "        data = data_1.get_fdata()\n",
    "        mask_1 = nib.load(seg_path)\n",
    "        mask = mask_1.get_fdata()\n",
    "        label = self.y_train[idx]\n",
    "\n",
    "        data = torch.from_numpy(data).unsqueeze(0)\n",
    "        mask = torch.from_numpy(mask).unsqueeze(0)\n",
    "        label = torch.tensor(label).long()\n",
    "\n",
    "        # Preprocess the data and mask\n",
    "        data, mask = preprocess_nifti_scan(img_path, self.threshold_value)\n",
    "\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        if self.target_transform:\n",
    "            mask = self.target_transform(mask)\n",
    "\n",
    "        return mask, data, label\n",
    "\n",
    "    def split_data(self):\n",
    "        X = self.img_labels.iloc[:, 0].values\n",
    "        y = self.img_labels.iloc[:, 2].values\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)\n",
    "\n",
    "        X_train = self.process_images(X_train)\n",
    "        X_test = self.process_images(X_test)\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def process_images(self, image_names):\n",
    "        image_tensors = []\n",
    "\n",
    "        for image_name in image_names:\n",
    "            # Load the mask directly\n",
    "            seg_path = os.path.join(self.img_dir, image_name, 'seg.nii.gz')\n",
    "            img_path = os.path.join(self.img_dir, image_name, 't1_gd.nii.gz')\n",
    "            data_1 = nib.load(img_path)\n",
    "            data = data_1.get_fdata()\n",
    "            mask_1 = nib.load(seg_path)\n",
    "            mask = mask_1.get_fdata()\n",
    "\n",
    "            data_tensor = torch.from_numpy(data).unsqueeze(0)\n",
    "            mask_tensor = torch.from_numpy(mask).unsqueeze(0)\n",
    "\n",
    "            image_tensors.append((data_tensor, mask_tensor))\n",
    "\n",
    "        return image_tensors\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = dataset.X_train, dataset.X_test, dataset.y_train, dataset.y_test\n",
    "X_train[0][1].sum(0).sum(1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T20:45:24.755328Z",
     "end_time": "2023-06-19T20:45:24.812926Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\r\n",
      "  Downloading opencv_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m61.8/61.8 MB\u001B[0m \u001B[31m41.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.19.3 in /home/amillanruiz/.conda/envs/secondenv/lib/python3.9/site-packages (from opencv-python) (1.24.2)\r\n",
      "Installing collected packages: opencv-python\r\n",
      "Successfully installed opencv-python-4.7.0.72\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-20T12:26:03.398717Z",
     "end_time": "2023-06-20T12:26:07.279418Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/projects/TMOR/data/Deeplearning/Oslo02/seg.nii.gz\n",
      "0 3190750\n",
      "1 47\n",
      "2 553\n",
      "3 1710\n",
      "4 26\n"
     ]
    }
   ],
   "source": [
    "seg_dir = '/data/projects/TMOR/data/Deeplearning/'\n",
    "for subj_id in os.listdir(seg_dir):\n",
    "    #extrasct the correct id from the subjects\n",
    "    #id = int(subj_id[7:9])\n",
    "    # Define paths to the segmentation and regular t1 postprocessed files for this subject\n",
    "    seg_glob = os.path.join(seg_dir, f'{subj_id}', f'seg.nii.gz')\n",
    "    reg_glob = os.path.join(seg_dir, f'{subj_id}', f't1_gd.nii.gz')\n",
    "    seg_paths = glob.glob(seg_glob)\n",
    "    reg_paths = glob.glob(reg_glob)\n",
    "    # Loop through all matching segmentation and registration files\n",
    "    for seg, reg in zip(seg_paths, reg_paths):\n",
    "        if os.path.exists(seg) and os.path.exists(reg):\n",
    "            print(seg)\n",
    "            mask = nib.load(seg)\n",
    "            vol_mask = mask.get_fdata()\n",
    "            klk = 0\n",
    "            for label in np.unique(vol_mask):\n",
    "                indices = np.argwhere(vol_mask == label)\n",
    "                print(klk, len(indices))\n",
    "                klk += 1\n",
    "                #print(indices)\n",
    "\n",
    "    break # Iterate over the indices\n",
    "            #for index in indices:\n",
    "                #row_idx, col_idx, slice_idx = index\n",
    "\n",
    "                # Access the index of seg_paths\n",
    "                #seg_index = seg_paths.index(seg)\n",
    "\n",
    "                # Do something with the index and indices\n",
    "                #print(f\"Index: {seg_index}, Coordinate: ({row_idx}, {col_idx}, {slice_idx})\")\n",
    "                #break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-20T16:06:59.261803Z",
     "end_time": "2023-06-20T16:06:59.523425Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "for label in np.unique(vol_mask):\n",
    "    print(label)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-20T15:53:13.118033Z",
     "end_time": "2023-06-20T15:53:13.203030Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "data": {
      "text/plain": "5"
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pene"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-20T16:05:44.850817Z",
     "end_time": "2023-06-20T16:05:44.854562Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) /io/opencv/modules/imgproc/src/connectedcomponents.cpp:5623: error: (-215:Assertion failed) L.channels() == 1 && I.channels() == 1 in function 'connectedComponents_sub1'\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31merror\u001B[0m                                     Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[71], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m threshold_value \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.5\u001B[39m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Call the get_tumor_bounding_boxes function\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m tumor_bounding_boxes \u001B[38;5;241m=\u001B[39m \u001B[43mget_tumor_bounding_boxes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mscan_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mthreshold_value\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Print the obtained tumor bounding boxes\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(tumor_bounding_boxes)\n",
      "Cell \u001B[0;32mIn[70], line 13\u001B[0m, in \u001B[0;36mget_tumor_bounding_boxes\u001B[0;34m(scan_data, threshold_value)\u001B[0m\n\u001B[1;32m     10\u001B[0m thresholded_scan \u001B[38;5;241m=\u001B[39m thresholded_scan\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39muint8)\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Perform connected component analysis to identify separate tumor regions\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m num_labels, labeled_scan \u001B[38;5;241m=\u001B[39m \u001B[43mcv2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnectedComponentsWithStats\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthresholded_scan\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(num_labels)\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# Iterate through each label (excluding the background label) and extract bounding boxes\u001B[39;00m\n",
      "\u001B[0;31merror\u001B[0m: OpenCV(4.7.0) /io/opencv/modules/imgproc/src/connectedcomponents.cpp:5623: error: (-215:Assertion failed) L.channels() == 1 && I.channels() == 1 in function 'connectedComponents_sub1'\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/data/projects/TMOR/data/Deeplearning/Oslo01/seg.nii.gz\"\n",
    "nifti_img = nib.load(file_path)\n",
    "scan_data = np.array(nifti_img.dataobj)\n",
    "#scan_data = nifti_img.get_fdata()\n",
    "threshold_value = 0.5\n",
    "\n",
    "# Call the get_tumor_bounding_boxes function\n",
    "tumor_bounding_boxes = get_tumor_bounding_boxes(scan_data, threshold_value)\n",
    "\n",
    "# Print the obtained tumor bounding boxes\n",
    "print(tumor_bounding_boxes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-20T12:23:53.357893Z",
     "end_time": "2023-06-20T12:23:53.402787Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 1], dtype=uint8)"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"/data/projects/TMOR/data/Deeplearning/Oslo01/seg.nii.gz\"\n",
    "nifti_img = nib.load(file_path)\n",
    "scan_data = np.array(nifti_img.dataobj)\n",
    "thresholded_scan = np.where(scan_data >= 0.5, 1, 0)\n",
    "thresholded_scan = thresholded_scan.astype(np.uint8)\n",
    "np.unique(thresholded_scan)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-20T12:52:02.115996Z",
     "end_time": "2023-06-20T12:52:02.200843Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) /io/opencv/modules/imgproc/src/connectedcomponents.cpp:5623: error: (-215:Assertion failed) L.channels() == 1 && I.channels() == 1 in function 'connectedComponents_sub1'\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31merror\u001B[0m                                     Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[27], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m thresholded_scan, num_labels, tumor_bounding_boxes \u001B[38;5;241m=\u001B[39m \u001B[43mget_tumor_bounding_boxes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mscan_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mthreshold_value\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(thresholded_scan)\n",
      "Cell \u001B[0;32mIn[24], line 13\u001B[0m, in \u001B[0;36mget_tumor_bounding_boxes\u001B[0;34m(scan_data, threshold_value)\u001B[0m\n\u001B[1;32m     10\u001B[0m thresholded_scan \u001B[38;5;241m=\u001B[39m thresholded_scan\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39muint8)\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Perform connected component analysis to identify separate tumor regions\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m num_labels, labeled_scan \u001B[38;5;241m=\u001B[39m \u001B[43mcv2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnectedComponents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthresholded_scan\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# Iterate through each label (excluding the background label) and extract bounding boxes\u001B[39;00m\n\u001B[1;32m     16\u001B[0m tumor_bounding_boxes \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[0;31merror\u001B[0m: OpenCV(4.7.0) /io/opencv/modules/imgproc/src/connectedcomponents.cpp:5623: error: (-215:Assertion failed) L.channels() == 1 && I.channels() == 1 in function 'connectedComponents_sub1'\n"
     ]
    }
   ],
   "source": [
    "thresholded_scan, num_labels, tumor_bounding_boxes = get_tumor_bounding_boxes(scan_data, threshold_value)\n",
    "\n",
    "print(thresholded_scan)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350\n"
     ]
    }
   ],
   "source": [
    "class BrainSegmentationDataset(Dataset):\n",
    "    def __init__(self, img_dir, labels_file, transform=None, target_transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_labels = pd.read_excel(labels_file)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = self.split_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.transform is None:\n",
    "            # Load the mask directly\n",
    "            seg_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0], 'seg.nii.gz')\n",
    "            img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0], 't1_gd.nii.gz')\n",
    "            label = self.y_train[idx]\n",
    "            data = nib.load(img_path).get_fdata()\n",
    "            mask = nib.load(seg_path).get_fdata()\n",
    "        else:\n",
    "            # Apply the transformation to the mask\n",
    "            seg_path = os.path.join(self.img_dinr, self.img_labels.iloc[idx, 0], 'seg.nii.gz')\n",
    "            img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0], 't1_gd.nii.gz')\n",
    "            label = self.y_train[idx]\n",
    "            mask = nib.load(seg_path).get_fdata()\n",
    "            data = nib.load(img_path).get_fdata()\n",
    "\n",
    "        mask = torch.from_numpy(mask).unsqueeze(0)\n",
    "        label = torch.tensor(label).long()\n",
    "\n",
    "        return mask, data,label\n",
    "\n",
    "    def split_data(self):\n",
    "        X = self.img_labels.iloc[:, 0].values\n",
    "        y = self.img_labels.iloc[:, 2].values\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)\n",
    "\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "img_dir = '/data/projects/TMOR/data/Deeplearning/'\n",
    "labels_file = 'labels_DL.xlsx'\n",
    "dataset = BrainSegmentationDataset(img_dir, labels_file)\n",
    "\n",
    "# Access the X_train list\n",
    "print(dataset.X_train.size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T19:05:32.452442Z",
     "end_time": "2023-06-19T19:05:32.490471Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "outputs": [],
   "source": [
    "img_dir = '/data/projects/TMOR/data/Deeplearning/'\n",
    "labels_file = 'labels_DL.xlsx'\n",
    "transform = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])\n",
    "\n",
    "dataset = BrainSegmentationDataset(img_dir,labels_file, transform=None)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T19:35:17.695217Z",
     "end_time": "2023-06-19T19:36:00.426919Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 135, 159, 132])"
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = dataset.X_train, dataset.X_test, dataset.y_train, dataset.y_test\n",
    "\n",
    "X_train[6].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T18:08:05.246161Z",
     "end_time": "2023-06-19T18:08:05.294773Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/projects/TMOR/data/Deeplearning/Oslo01/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo01/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo02/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo02/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo03/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo03/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo04/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo06/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo07/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo08/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo09/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo09/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo10/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo11/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo12/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo13/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo14/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo17/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo18/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo18/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo18/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo19/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo20/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo20/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo21/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo22/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo23/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo24/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo24/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo25/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo26/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo26/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo28/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo29/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo30/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo30/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo31/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo31/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo32/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo32/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo33/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo33/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo34/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo35/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo36/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo37/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo37/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo38/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo38/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo39/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo40/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo41/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo41/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo42/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo43/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo43/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo43/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo44/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo45/seg.nii.gz\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[403], line 31\u001B[0m\n\u001B[1;32m     27\u001B[0m     image_tensors\u001B[38;5;241m.\u001B[39mappend((data_tensor, mask_tensor))\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m image_tensors\n\u001B[0;32m---> 31\u001B[0m processed_images \u001B[38;5;241m=\u001B[39m \u001B[43mprocess_images\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels_file\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[403], line 21\u001B[0m, in \u001B[0;36mprocess_images\u001B[0;34m(img_dir, labels_file)\u001B[0m\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;28mprint\u001B[39m(seg_path)\n\u001B[1;32m     20\u001B[0m     img_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(img_dir, image_name, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt1_gd.nii.gz\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 21\u001B[0m     mask \u001B[38;5;241m=\u001B[39m \u001B[43mnib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseg_path\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_fdata\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m     data \u001B[38;5;241m=\u001B[39m nib\u001B[38;5;241m.\u001B[39mload(img_path)\u001B[38;5;241m.\u001B[39mget_fdata()\n\u001B[1;32m     24\u001B[0m data_tensor \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(data)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/site-packages/nibabel/dataobj_images.py:368\u001B[0m, in \u001B[0;36mDataobjImage.get_fdata\u001B[0;34m(self, caching, dtype)\u001B[0m\n\u001B[1;32m    364\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fdata_cache\n\u001B[1;32m    365\u001B[0m \u001B[38;5;66;03m# Always return requested data type\u001B[39;00m\n\u001B[1;32m    366\u001B[0m \u001B[38;5;66;03m# For array proxies, will attempt to confine data array to dtype\u001B[39;00m\n\u001B[1;32m    367\u001B[0m \u001B[38;5;66;03m# during scaling\u001B[39;00m\n\u001B[0;32m--> 368\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43masanyarray\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    369\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m caching \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfill\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    370\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fdata_cache \u001B[38;5;241m=\u001B[39m data\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/site-packages/nibabel/arrayproxy.py:426\u001B[0m, in \u001B[0;36mArrayProxy.__array__\u001B[0;34m(self, dtype)\u001B[0m\n\u001B[1;32m    405\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__array__\u001B[39m(\u001B[38;5;28mself\u001B[39m, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    406\u001B[0m     \u001B[38;5;124;03m\"\"\"Read data from file and apply scaling, casting to ``dtype``\u001B[39;00m\n\u001B[1;32m    407\u001B[0m \n\u001B[1;32m    408\u001B[0m \u001B[38;5;124;03m    If ``dtype`` is unspecified, the dtype of the returned array is the\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    424\u001B[0m \u001B[38;5;124;03m        Scaled image data with type `dtype`.\u001B[39;00m\n\u001B[1;32m    425\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 426\u001B[0m     arr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_scaled\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mslicer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    427\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    428\u001B[0m         arr \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mastype(dtype, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/site-packages/nibabel/arrayproxy.py:393\u001B[0m, in \u001B[0;36mArrayProxy._get_scaled\u001B[0;34m(self, dtype, slicer)\u001B[0m\n\u001B[1;32m    391\u001B[0m     scl_inter \u001B[38;5;241m=\u001B[39m scl_inter\u001B[38;5;241m.\u001B[39mastype(use_dtype)\n\u001B[1;32m    392\u001B[0m \u001B[38;5;66;03m# Read array and upcast as necessary for big slopes, intercepts\u001B[39;00m\n\u001B[0;32m--> 393\u001B[0m scaled \u001B[38;5;241m=\u001B[39m apply_read_scaling(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_unscaled\u001B[49m\u001B[43m(\u001B[49m\u001B[43mslicer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mslicer\u001B[49m\u001B[43m)\u001B[49m, scl_slope, scl_inter)\n\u001B[1;32m    394\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    395\u001B[0m     scaled \u001B[38;5;241m=\u001B[39m scaled\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mpromote_types(scaled\u001B[38;5;241m.\u001B[39mdtype, dtype), copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/site-packages/nibabel/arrayproxy.py:363\u001B[0m, in \u001B[0;36mArrayProxy._get_unscaled\u001B[0;34m(self, slicer)\u001B[0m\n\u001B[1;32m    359\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m canonical_slicers(slicer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shape, \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m==\u001B[39m canonical_slicers(\n\u001B[1;32m    360\u001B[0m     (), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shape, \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    361\u001B[0m ):\n\u001B[1;32m    362\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_fileobj() \u001B[38;5;28;01mas\u001B[39;00m fileobj, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m--> 363\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43marray_from_file\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    364\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_shape\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    365\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    366\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfileobj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    367\u001B[0m \u001B[43m            \u001B[49m\u001B[43moffset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_offset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    368\u001B[0m \u001B[43m            \u001B[49m\u001B[43morder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43morder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    369\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmmap\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_mmap\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    370\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    371\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_fileobj() \u001B[38;5;28;01mas\u001B[39;00m fileobj:\n\u001B[1;32m    372\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fileslice(\n\u001B[1;32m    373\u001B[0m         fileobj,\n\u001B[1;32m    374\u001B[0m         slicer,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    379\u001B[0m         lock\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock,\n\u001B[1;32m    380\u001B[0m     )\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/site-packages/nibabel/volumeutils.py:454\u001B[0m, in \u001B[0;36marray_from_file\u001B[0;34m(shape, in_dtype, infile, offset, order, mmap)\u001B[0m\n\u001B[1;32m    452\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(infile, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreadinto\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m    453\u001B[0m     data_bytes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mbytearray\u001B[39m(n_bytes)\n\u001B[0;32m--> 454\u001B[0m     n_read \u001B[38;5;241m=\u001B[39m \u001B[43minfile\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadinto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_bytes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    455\u001B[0m     needs_copy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    456\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/gzip.py:300\u001B[0m, in \u001B[0;36mGzipFile.read\u001B[0;34m(self, size)\u001B[0m\n\u001B[1;32m    298\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01merrno\u001B[39;00m\n\u001B[1;32m    299\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(errno\u001B[38;5;241m.\u001B[39mEBADF, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mread() on write-only GzipFile object\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 300\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_buffer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43msize\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/_compression.py:68\u001B[0m, in \u001B[0;36mDecompressReader.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreadinto\u001B[39m(\u001B[38;5;28mself\u001B[39m, b):\n\u001B[1;32m     67\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mmemoryview\u001B[39m(b) \u001B[38;5;28;01mas\u001B[39;00m view, view\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mB\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m byte_view:\n\u001B[0;32m---> 68\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mbyte_view\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     69\u001B[0m         byte_view[:\u001B[38;5;28mlen\u001B[39m(data)] \u001B[38;5;241m=\u001B[39m data\n\u001B[1;32m     70\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(data)\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/gzip.py:509\u001B[0m, in \u001B[0;36m_GzipReader.read\u001B[0;34m(self, size)\u001B[0m\n\u001B[1;32m    505\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m buf \u001B[38;5;241m==\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    506\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEOFError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCompressed file ended before the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    507\u001B[0m                        \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mend-of-stream marker was reached\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 509\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_add_read_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43muncompress\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    510\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pos \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(uncompress)\n\u001B[1;32m    511\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m uncompress\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/gzip.py:514\u001B[0m, in \u001B[0;36m_GzipReader._add_read_data\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m    513\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_add_read_data\u001B[39m(\u001B[38;5;28mself\u001B[39m, data):\n\u001B[0;32m--> 514\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_crc \u001B[38;5;241m=\u001B[39m \u001B[43mzlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcrc32\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_crc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    515\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stream_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stream_size \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mlen\u001B[39m(data)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame called \"data\"\n",
    "# Features are stored in columns X1, X2, X3, ...\n",
    "# The target variable is stored in column 'y'\n",
    "\n",
    "X = new_df.drop(['Labels', 'Tumor'],axis=1)  # Features\n",
    "y = new_df['Labels']  # Target variable\n",
    "# Random split into 80% train and 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train\n",
    "\n",
    "img_dir = '/data/projects/TMOR/data/Deeplearning/'\n",
    "labels_file = pd.read_excel('labels_DL.xlsx')\n",
    "\n",
    "def process_images(img_dir, labels_file):\n",
    "    image_tensors = []\n",
    "    pene = list(labels_file.iloc[:, 0])\n",
    "    for image_name in pene:\n",
    "        seg_path = os.path.join(img_dir, image_name, 'seg.nii.gz')\n",
    "        print(seg_path)\n",
    "        img_path = os.path.join(img_dir, image_name, 't1_gd.nii.gz')\n",
    "        mask = nib.load(seg_path).get_fdata()\n",
    "        data = nib.load(img_path).get_fdata()\n",
    "\n",
    "    data_tensor = torch.from_numpy(data).unsqueeze(0)\n",
    "    mask_tensor = torch.from_numpy(mask).unsqueeze(0)\n",
    "\n",
    "    image_tensors.append((data_tensor, mask_tensor))\n",
    "\n",
    "    return image_tensors\n",
    "\n",
    "processed_images = process_images(img_dir, labels_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T18:44:25.135265Z",
     "end_time": "2023-06-19T18:44:55.162618Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stan_265\n",
      "Oslo62\n",
      "Stan_197\n",
      "Stan_058\n",
      "Stan_319\n",
      "Oslo47\n",
      "Stan_026\n",
      "Stan_165\n",
      "Oslo31\n",
      "Stan_260\n",
      "Stan_049\n",
      "Stan_312\n",
      "Oslo41\n",
      "Stan_045\n",
      "Oslo26\n",
      "Stan_251\n",
      "Stan_285\n",
      "Oslo13\n",
      "Stan_051\n",
      "Stan_028\n",
      "Stan_183\n",
      "Oslo32\n",
      "Stan_064\n",
      "Stan_203\n",
      "Oslo46\n",
      "Oslo06\n",
      "Oslo37\n",
      "Stan_009\n",
      "Stan_315\n",
      "Stan_024\n",
      "Oslo20\n",
      "Stan_184\n",
      "Oslo12\n",
      "Stan_126\n",
      "Stan_120\n",
      "Oslo01\n",
      "Stan_144\n",
      "Stan_019\n",
      "Oslo49\n",
      "Stan_054\n",
      "Oslo33\n",
      "Oslo45\n",
      "Stan_291\n",
      "Oslo30\n",
      "Oslo28\n",
      "Stan_134\n",
      "Oslo44\n",
      "Stan_059\n",
      "Stan_102\n",
      "Stan_318\n",
      "Stan_246\n",
      "Oslo52\n",
      "Stan_148\n",
      "Stan_010\n",
      "Stan_038\n",
      "Oslo38\n",
      "Stan_121\n",
      "Stan_136\n",
      "Stan_171\n",
      "Stan_014\n",
      "Stan_087\n",
      "Stan_098\n",
      "Oslo60\n",
      "Oslo07\n",
      "Stan_132\n",
      "Stan_170\n",
      "Oslo54\n",
      "Oslo40\n",
      "Stan_052\n",
      "Stan_047\n",
      "Oslo39\n",
      "Oslo51\n",
      "Oslo09\n",
      "Oslo48\n",
      "Stan_033\n",
      "Stan_307\n",
      "Stan_072\n",
      "Stan_055\n",
      "Stan_289\n",
      "Stan_290\n",
      "Stan_068\n",
      "Oslo10\n",
      "Stan_142\n",
      "Oslo17\n",
      "Stan_005\n",
      "Stan_257\n",
      "Oslo04\n",
      "Oslo21\n",
      "Oslo43\n",
      "Stan_037\n",
      "Stan_266\n",
      "Oslo08\n",
      "Stan_244\n",
      "Stan_101\n",
      "Oslo03\n",
      "Stan_173\n",
      "Stan_016\n",
      "Oslo61\n",
      "Stan_123\n",
      "Oslo55\n",
      "Stan_323\n",
      "Oslo66\n",
      "Oslo53\n",
      "Stan_066\n",
      "Oslo25\n",
      "Oslo65\n",
      "Stan_316\n",
      "Stan_227\n",
      "Oslo42\n",
      "Stan_311\n",
      "Oslo02\n",
      "Oslo59\n",
      "Stan_230\n",
      "Stan_111\n",
      "Stan_100\n",
      "Stan_149\n",
      "Stan_065\n",
      "Stan_039\n",
      "Stan_176\n",
      "Stan_338\n",
      "Oslo24\n",
      "Stan_036\n",
      "Stan_122\n",
      "Oslo18\n",
      "Stan_074\n",
      "Stan_107\n"
     ]
    }
   ],
   "source": [
    "pene = list(X_train.iloc[:, 0])\n",
    "\n",
    "for i in pene:\n",
    "    print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T18:59:48.543659Z",
     "end_time": "2023-06-19T18:59:48.569369Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "outputs": [
    {
     "data": {
      "text/plain": "Subjid    Stan_319\nName: 2369, dtype: object"
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[4, :]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T18:55:18.616208Z",
     "end_time": "2023-06-19T18:55:18.656019Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
