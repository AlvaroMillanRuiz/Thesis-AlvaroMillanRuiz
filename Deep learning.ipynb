{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-07-06T11:31:23.278027Z",
     "end_time": "2023-07-06T11:31:25.586916Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import glob\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "        subjid  label  shape_Elongation  shape_Flatness   \n0       Oslo01      1          0.829356        0.682981  \\\n1       Oslo01      2          0.944143        0.900681   \n4       Oslo02      4          0.849295        0.841082   \n5       Oslo02      5          0.939828        0.821847   \n7       Oslo03      1          0.930698        0.629604   \n...        ...    ...               ...             ...   \n2408  Stan_328      1          0.810571        0.325076   \n2415  Stan_328      8          0.967496        0.597282   \n2418  Stan_328     11          0.561746        0.506390   \n2419  Stan_328     12          0.959775        0.756903   \n2427  Stan_338      4          0.819799        0.618051   \n\n      shape_LeastAxisLength  shape_MajorAxisLength   \n0                  8.512717              12.464063  \\\n1                 14.133927              15.692487   \n4                  8.703783              10.348315   \n5                 12.093804              14.715396   \n7                 10.914986              17.336268   \n...                     ...                    ...   \n2408               8.769420              26.976501   \n2415               6.131668              10.265944   \n2418               5.004740               9.883181   \n2419               5.164755               6.823538   \n2427               8.548537              13.831433   \n\n      shape_Maximum2DDiameterColumn  shape_Maximum2DDiameterRow   \n0                         14.866069                   14.560220  \\\n1                         17.804494                   19.416488   \n4                         10.816654                   11.704700   \n5                         17.262677                   17.888544   \n7                         20.615528                   18.439089   \n...                             ...                         ...   \n2408                      31.384710                   28.160256   \n2415                      11.661904                   12.369317   \n2418                       8.246211                   11.704700   \n2419                       7.615773                    8.602325   \n2427                      14.142136                   15.297059   \n\n      shape_Maximum2DDiameterSlice  shape_Maximum3DDiameter  ...   \n0                        12.529964                15.394804  ...  \\\n1                        18.681542                19.467922  ...   \n4                        11.704700                12.449900  ...   \n5                        17.492856                18.000000  ...   \n7                        19.416488                20.712315  ...   \n...                            ...                      ...  ...   \n2408                     27.459060                33.075671  ...   \n2415                     12.165525                12.688578  ...   \n2418                     10.000000                12.124356  ...   \n2419                      8.602325                 8.602325  ...   \n2427                     17.204651                17.291616  ...   \n\n      ngtdm_Busyness  ngtdm_Coarseness  ngtdm_Complexity  ngtdm_Contrast   \n0         158.977235          0.008929          0.302273        0.074659  \\\n1          39.445794          0.007407          0.115448        0.010426   \n4          14.239428          0.022868          0.159307        0.020316   \n5          49.609354          0.008536          0.137901        0.028455   \n7          43.648474          0.007781          0.123220        0.017415   \n...              ...               ...               ...             ...   \n2408       12.988427          0.019891          0.030104        0.000639   \n2415       18.452502          0.024791          0.208465        0.046975   \n2418        0.553323          0.468447          0.025263        0.000748   \n2419        0.000000    1000000.000000          0.000000        0.000000   \n2427        6.977967          0.040814          0.057854        0.005610   \n\n      ngtdm_Strength  Age  number_annotations  F  M  labels  \n0           0.008950   58                   1  0  1       0  \n1           0.007130   58                   1  0  1       0  \n4           0.023465   50                   1  1  0       1  \n5           0.008212   50                   1  1  0       1  \n7           0.007982   64                   1  0  1       0  \n...              ...  ...                 ... .. ..     ...  \n2408        0.019768   53                   1  0  1       1  \n2415        0.023205   53                   1  0  1       1  \n2418        0.365681   53                   1  0  1       1  \n2419        0.000000   53                   1  0  1       1  \n2427        0.031493   42                   1  1  0       1  \n\n[438 rows x 114 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subjid</th>\n      <th>label</th>\n      <th>shape_Elongation</th>\n      <th>shape_Flatness</th>\n      <th>shape_LeastAxisLength</th>\n      <th>shape_MajorAxisLength</th>\n      <th>shape_Maximum2DDiameterColumn</th>\n      <th>shape_Maximum2DDiameterRow</th>\n      <th>shape_Maximum2DDiameterSlice</th>\n      <th>shape_Maximum3DDiameter</th>\n      <th>...</th>\n      <th>ngtdm_Busyness</th>\n      <th>ngtdm_Coarseness</th>\n      <th>ngtdm_Complexity</th>\n      <th>ngtdm_Contrast</th>\n      <th>ngtdm_Strength</th>\n      <th>Age</th>\n      <th>number_annotations</th>\n      <th>F</th>\n      <th>M</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Oslo01</td>\n      <td>1</td>\n      <td>0.829356</td>\n      <td>0.682981</td>\n      <td>8.512717</td>\n      <td>12.464063</td>\n      <td>14.866069</td>\n      <td>14.560220</td>\n      <td>12.529964</td>\n      <td>15.394804</td>\n      <td>...</td>\n      <td>158.977235</td>\n      <td>0.008929</td>\n      <td>0.302273</td>\n      <td>0.074659</td>\n      <td>0.008950</td>\n      <td>58</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Oslo01</td>\n      <td>2</td>\n      <td>0.944143</td>\n      <td>0.900681</td>\n      <td>14.133927</td>\n      <td>15.692487</td>\n      <td>17.804494</td>\n      <td>19.416488</td>\n      <td>18.681542</td>\n      <td>19.467922</td>\n      <td>...</td>\n      <td>39.445794</td>\n      <td>0.007407</td>\n      <td>0.115448</td>\n      <td>0.010426</td>\n      <td>0.007130</td>\n      <td>58</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Oslo02</td>\n      <td>4</td>\n      <td>0.849295</td>\n      <td>0.841082</td>\n      <td>8.703783</td>\n      <td>10.348315</td>\n      <td>10.816654</td>\n      <td>11.704700</td>\n      <td>11.704700</td>\n      <td>12.449900</td>\n      <td>...</td>\n      <td>14.239428</td>\n      <td>0.022868</td>\n      <td>0.159307</td>\n      <td>0.020316</td>\n      <td>0.023465</td>\n      <td>50</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Oslo02</td>\n      <td>5</td>\n      <td>0.939828</td>\n      <td>0.821847</td>\n      <td>12.093804</td>\n      <td>14.715396</td>\n      <td>17.262677</td>\n      <td>17.888544</td>\n      <td>17.492856</td>\n      <td>18.000000</td>\n      <td>...</td>\n      <td>49.609354</td>\n      <td>0.008536</td>\n      <td>0.137901</td>\n      <td>0.028455</td>\n      <td>0.008212</td>\n      <td>50</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Oslo03</td>\n      <td>1</td>\n      <td>0.930698</td>\n      <td>0.629604</td>\n      <td>10.914986</td>\n      <td>17.336268</td>\n      <td>20.615528</td>\n      <td>18.439089</td>\n      <td>19.416488</td>\n      <td>20.712315</td>\n      <td>...</td>\n      <td>43.648474</td>\n      <td>0.007781</td>\n      <td>0.123220</td>\n      <td>0.017415</td>\n      <td>0.007982</td>\n      <td>64</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2408</th>\n      <td>Stan_328</td>\n      <td>1</td>\n      <td>0.810571</td>\n      <td>0.325076</td>\n      <td>8.769420</td>\n      <td>26.976501</td>\n      <td>31.384710</td>\n      <td>28.160256</td>\n      <td>27.459060</td>\n      <td>33.075671</td>\n      <td>...</td>\n      <td>12.988427</td>\n      <td>0.019891</td>\n      <td>0.030104</td>\n      <td>0.000639</td>\n      <td>0.019768</td>\n      <td>53</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2415</th>\n      <td>Stan_328</td>\n      <td>8</td>\n      <td>0.967496</td>\n      <td>0.597282</td>\n      <td>6.131668</td>\n      <td>10.265944</td>\n      <td>11.661904</td>\n      <td>12.369317</td>\n      <td>12.165525</td>\n      <td>12.688578</td>\n      <td>...</td>\n      <td>18.452502</td>\n      <td>0.024791</td>\n      <td>0.208465</td>\n      <td>0.046975</td>\n      <td>0.023205</td>\n      <td>53</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2418</th>\n      <td>Stan_328</td>\n      <td>11</td>\n      <td>0.561746</td>\n      <td>0.506390</td>\n      <td>5.004740</td>\n      <td>9.883181</td>\n      <td>8.246211</td>\n      <td>11.704700</td>\n      <td>10.000000</td>\n      <td>12.124356</td>\n      <td>...</td>\n      <td>0.553323</td>\n      <td>0.468447</td>\n      <td>0.025263</td>\n      <td>0.000748</td>\n      <td>0.365681</td>\n      <td>53</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2419</th>\n      <td>Stan_328</td>\n      <td>12</td>\n      <td>0.959775</td>\n      <td>0.756903</td>\n      <td>5.164755</td>\n      <td>6.823538</td>\n      <td>7.615773</td>\n      <td>8.602325</td>\n      <td>8.602325</td>\n      <td>8.602325</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>1000000.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>53</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2427</th>\n      <td>Stan_338</td>\n      <td>4</td>\n      <td>0.819799</td>\n      <td>0.618051</td>\n      <td>8.548537</td>\n      <td>13.831433</td>\n      <td>14.142136</td>\n      <td>15.297059</td>\n      <td>17.204651</td>\n      <td>17.291616</td>\n      <td>...</td>\n      <td>6.977967</td>\n      <td>0.040814</td>\n      <td>0.057854</td>\n      <td>0.005610</td>\n      <td>0.031493</td>\n      <td>42</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>438 rows × 114 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('all_patients.xlsx')\n",
    "df = df[df.shape_VoxelVolume > 125]\n",
    "df.drop(df[df['labels'] == 3].index, inplace = True) ## dropping label of the call OTHERS\n",
    "df.drop(df[df['number_annotations'] > 1].index, inplace = True)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T11:31:28.488175Z",
     "end_time": "2023-07-06T11:31:31.320702Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "        Subjid  Tumor  Labels\n0       Oslo01      1       0\n4       Oslo02      4       1\n7       Oslo03      1       0\n11      Oslo04      1       0\n13      Oslo06      1       1\n...        ...    ...     ...\n2359  Stan_318      1       1\n2369  Stan_319      2       1\n2387  Stan_323      3       1\n2408  Stan_328      1       1\n2427  Stan_338      4       1\n\n[158 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Subjid</th>\n      <th>Tumor</th>\n      <th>Labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Oslo01</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Oslo02</td>\n      <td>4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Oslo03</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Oslo04</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Oslo06</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2359</th>\n      <td>Stan_318</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2369</th>\n      <td>Stan_319</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2387</th>\n      <td>Stan_323</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2408</th>\n      <td>Stan_328</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2427</th>\n      <td>Stan_338</td>\n      <td>4</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>158 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.DataFrame({'Subjid': df.iloc[:, 0], 'Tumor': df.iloc[:, 1], 'Labels': df.iloc[:, -1]})\n",
    "new_df = new_df.drop_duplicates(subset='Subjid', keep='first')\n",
    "#new_df.to_excel('labels_DL.xlsx', index = False)\n",
    "new_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T11:31:41.793869Z",
     "end_time": "2023-07-06T11:31:41.797178Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "stan = new_df['Subjid'][59:].to_list()\n",
    "oslo = new_df['Subjid'][:59].to_list()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T11:31:49.211766Z",
     "end_time": "2023-07-06T11:31:49.215024Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## we have created a new folder called deep learning with all the folder with all the folder per patient labels as 0, 1 or 2 with tumors greater than 125mm3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "outputs": [],
   "source": [
    "# Specify the path to the parent folder\n",
    "parent_folder = '/data/projects/TMOR/data/Deeplearning/'\n",
    "\n",
    "# Get a list of all subfolders within the parent folder\n",
    "subfolders = [f.path for f in os.scandir(parent_folder) if f.is_dir()]\n",
    "\n",
    "# Iterate over the subfolders and rename them\n",
    "\n",
    "for folder in subfolders:\n",
    "    folder_name = os.path.basename(folder)\n",
    "    new_folder_name = folder_name.replace(\"Subject\", \"Oslo\")\n",
    "    #new_folder_name = folder_name.replace(\"Mets\", \"Stan\")\n",
    "    new_folder_path = os.path.join(parent_folder, new_folder_name)\n",
    "    os.rename(folder, new_folder_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T17:47:59.313418Z",
     "end_time": "2023-06-19T17:47:59.341160Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "outputs": [],
   "source": [
    "#delete the other file for OSLO\n",
    "directory = '/data/projects/TMOR/data/Deeplearning/'\n",
    "\n",
    "# Get a list of all patient folders\n",
    "patient_folders = glob.glob(os.path.join(directory, 'Oslo*'))\n",
    "\n",
    "for patient_folder in patient_folders:\n",
    "    #id = patient_folder[-2:]\n",
    "    seg_folders = glob.glob(os.path.join(patient_folder, 'seg*'))\n",
    "    t1_gds = glob.glob(os.path.join(patient_folder, 't1_gd*'))\n",
    "    #print(seg_folders)\n",
    "    # Iterate over each seg folder\n",
    "    for seg_folder in seg_folders:\n",
    "        for t1_gd in t1_gds:\n",
    "            #print(seg_folder)\n",
    "            image_files = glob.glob(os.path.join(patient_folder, '*'))\n",
    "            #print(image_files)\n",
    "            # Delete all the image files except for 'seg_1'\n",
    "            for file in image_files:\n",
    "            # Check if the file is in the segmentation folder or T1-Gd folder\n",
    "                if file not in [seg_folder, t1_gd]:\n",
    "                    os.remove(file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T17:48:07.240665Z",
     "end_time": "2023-06-19T17:48:07.380290Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "outputs": [],
   "source": [
    "patient_folders = glob.glob(os.path.join(directory, 'Oslo*'))\n",
    "import shutil\n",
    "for patient_folder in patient_folders:\n",
    "    id = patient_folder[-6:]\n",
    "    if id not in oslo:\n",
    "        shutil.rmtree(patient_folder)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T17:48:30.041220Z",
     "end_time": "2023-06-19T17:48:30.067252Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "directory = '/data/projects/TMOR/data/Deeplearning/'\n",
    "\n",
    "patient_folders = glob.glob(os.path.join(directory, 'Oslo*'))\n",
    "\n",
    "\n",
    "# Iterate over patient folders\n",
    "for patient_folder in patient_folders:\n",
    "    #patient_path = os.path.join(directory, patient_folder)\n",
    "    seg_folders = glob.glob(os.path.join(patient_folder, 'seg*'))\n",
    "    # Rename segmentation files\n",
    "    for seg_folder in seg_folders:\n",
    "        seg_old_path = seg_folder\n",
    "        seg_new_path = os.path.join(patient_folder, 'seg.nii.gz')\n",
    "        os.rename(seg_old_path, seg_new_path)\n",
    "        #print(seg_new_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-20T21:41:11.352413Z",
     "end_time": "2023-06-20T21:41:11.359538Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## STANDFORD DATA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "outputs": [],
   "source": [
    "# Specify the path to the parent folder\n",
    "parent_folder = '/data/projects/TMOR/data/Deeplearning/'\n",
    "\n",
    "# Get a list of all subfolders within the parent folder\n",
    "subfolders = [f.path for f in os.scandir(parent_folder) if f.is_dir()]\n",
    "\n",
    "# Iterate over the subfolders and rename them\n",
    "\n",
    "for folder in subfolders:\n",
    "    folder_name = os.path.basename(folder)\n",
    "    new_folder_name = folder_name.replace(\"Mets\", \"Stan\")\n",
    "    new_folder_path = os.path.join(parent_folder, new_folder_name)\n",
    "    os.rename(folder, new_folder_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T17:51:09.426017Z",
     "end_time": "2023-06-19T17:51:09.465564Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "#delete the other file for OSLO\n",
    "directory = '/data/projects/TMOR/data/Deeplearning/'\n",
    "\n",
    "# Get a list of all patient folders\n",
    "patient_folders = glob.glob(os.path.join(directory, 'Stan*'))\n",
    "\n",
    "for patient_folder in patient_folders:\n",
    "    seg_folders = glob.glob(os.path.join(patient_folder, 'seg*'))\n",
    "    t1_gds = glob.glob(os.path.join(patient_folder, 't1_gd*'))\n",
    "    # Iterate over each seg folder\n",
    "    for seg_folder in seg_folders:\n",
    "        for t1_gd in t1_gds:\n",
    "            image_files = glob.glob(os.path.join(patient_folder, '*'))\n",
    "            # Delete all the image files except for 'seg_1'\n",
    "            for file in image_files:\n",
    "                # Check if the file is in the segmentation folder or T1-Gd folder\n",
    "                if file not in [seg_folder, t1_gd]:\n",
    "                    os.remove(file)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T17:51:13.837773Z",
     "end_time": "2023-06-19T17:51:14.245613Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "outputs": [],
   "source": [
    "#Remove all the files that are not within stan\n",
    "import shutil\n",
    "for patient_folder in patient_folders:\n",
    "    id = patient_folder[-8:]\n",
    "    if id not in stan:\n",
    "        shutil.rmtree(patient_folder)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T17:51:18.497052Z",
     "end_time": "2023-06-19T17:51:18.561806Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## move all t1_pre to the right folder of /data/projects/TMOR/data/Deeplearning/A/"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## OSLO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# Specify the path to the parent folder\n",
    "parent_folder = '/data/projects/TMOR/data/OsloPreprocessed/OsloPreprocessed/'\n",
    "\n",
    "# Get a list of all subfolders within the parent folder\n",
    "subfolders = [f.path for f in os.scandir(parent_folder) if f.is_dir()]\n",
    "\n",
    "# Iterate over the subfolders and rename them\n",
    "\n",
    "for folder in subfolders:\n",
    "    folder_name = os.path.basename(folder)\n",
    "    new_folder_name = folder_name.replace(\"Subject\", \"Oslo\")\n",
    "    #new_folder_name = folder_name.replace(\"Mets\", \"Stan\")\n",
    "    new_folder_path = os.path.join(parent_folder, new_folder_name)\n",
    "    os.rename(folder, new_folder_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T11:55:52.166448Z",
     "end_time": "2023-07-06T11:55:52.181085Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "import shutil\n",
    "directory = '/data/projects/TMOR/data/OsloPreprocessed/OsloPreprocessed/'\n",
    "\n",
    "patient_folders = glob.glob(os.path.join(directory, 'Oslo*'))\n",
    "\n",
    "repository = '/data/projects/TMOR/data/Deeplearning/'\n",
    "\n",
    "pacientes = glob.glob(os.path.join(repository, 'Oslo*'))\n",
    "\n",
    "for patient_folder in patient_folders:\n",
    "    t1_pre_files = glob.glob(os.path.join(patient_folder, 't1_pre*'))\n",
    "    for t1 in t1_pre_files:\n",
    "        patient_id = os.path.basename(patient_folder)\n",
    "        patient_match = next((p for p in pacientes if patient_id in p), None)\n",
    "        if patient_match:\n",
    "        # Move each t1-pre file to the respective patient folder\n",
    "            for t1_pre_file in t1_pre_files:\n",
    "                destination_folder = os.path.join(patient_match, os.path.basename(t1_pre_file))\n",
    "                shutil.copy(t1_pre_file, destination_folder)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T12:08:01.073832Z",
     "end_time": "2023-07-06T12:08:02.492279Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Standford"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# Specify the path to the parent folder\n",
    "parent_folder = '/data/projects/TMOR/data/StanfordPreprocessed/StanfordPreprocessed/'\n",
    "\n",
    "# Get a list of all subfolders within the parent folder\n",
    "subfolders = [f.path for f in os.scandir(parent_folder) if f.is_dir()]\n",
    "\n",
    "# Iterate over the subfolders and rename them\n",
    "\n",
    "for folder in subfolders:\n",
    "    folder_name = os.path.basename(folder)\n",
    "    #new_folder_name = folder_name.replace(\"Subject\", \"Oslo\")\n",
    "    new_folder_name = folder_name.replace(\"Mets\", \"Stan\")\n",
    "    new_folder_path = os.path.join(parent_folder, new_folder_name)\n",
    "    os.rename(folder, new_folder_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T12:15:15.366395Z",
     "end_time": "2023-07-06T12:15:15.406773Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "directory = '/data/projects/TMOR/data/StanfordPreprocessed/StanfordPreprocessed/'\n",
    "\n",
    "patient_folders = glob.glob(os.path.join(directory, 'Stan*'))\n",
    "\n",
    "repository = '/data/projects/TMOR/data/Deeplearning/'\n",
    "\n",
    "pacientes = glob.glob(os.path.join(repository, 'Stan*'))\n",
    "\n",
    "for patient_folder in patient_folders:\n",
    "    t1_pre_files = glob.glob(os.path.join(patient_folder, 't1_pre*'))\n",
    "    for t1 in t1_pre_files:\n",
    "        patient_id = os.path.basename(patient_folder)\n",
    "        patient_match = next((p for p in pacientes if patient_id in p), None)\n",
    "        if patient_match:\n",
    "        # Move each t1-pre file to the respective patient folder\n",
    "            for t1_pre_file in t1_pre_files:\n",
    "                destination_folder = os.path.join(patient_match, os.path.basename(t1_pre_file))\n",
    "                shutil.copy(t1_pre_file, destination_folder)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T12:16:31.730909Z",
     "end_time": "2023-07-06T12:16:33.740620Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## class object"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1344\n",
      "1344\n",
      "0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[34], line 103\u001B[0m\n\u001B[1;32m    100\u001B[0m labels_file \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels_DL.xlsx\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    101\u001B[0m transform \u001B[38;5;241m=\u001B[39m transforms\u001B[38;5;241m.\u001B[39mCompose([transforms\u001B[38;5;241m.\u001B[39mResize((\u001B[38;5;241m256\u001B[39m, \u001B[38;5;241m256\u001B[39m)), transforms\u001B[38;5;241m.\u001B[39mToTensor()])\n\u001B[0;32m--> 103\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mBrainSegmentationDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransform\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    104\u001B[0m data_loader \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataLoader(dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m16\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "Cell \u001B[0;32mIn[34], line 21\u001B[0m, in \u001B[0;36mBrainSegmentationDataset.__init__\u001B[0;34m(self, img_dir, labels_file, transform, target_transform)\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;241m=\u001B[39m transform\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform \u001B[38;5;241m=\u001B[39m target_transform\n\u001B[0;32m---> 21\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mX_train, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mX_test, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39my_train, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39my_test \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[34], line 61\u001B[0m, in \u001B[0;36mBrainSegmentationDataset.split_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     57\u001B[0m y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimg_labels\u001B[38;5;241m.\u001B[39miloc[:, \u001B[38;5;241m2\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues\n\u001B[1;32m     59\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m train_test_split(X, y, test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m, stratify\u001B[38;5;241m=\u001B[39my, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m---> 61\u001B[0m X_train \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess_images\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     62\u001B[0m X_test \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_images(X_test)\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m X_train, X_test, y_train, y_test\n",
      "Cell \u001B[0;32mIn[34], line 89\u001B[0m, in \u001B[0;36mBrainSegmentationDataset.process_images\u001B[0;34m(self, image_names)\u001B[0m\n\u001B[1;32m     86\u001B[0m end \u001B[38;5;241m=\u001B[39m start \u001B[38;5;241m+\u001B[39m max_tumor_size\n\u001B[1;32m     88\u001B[0m \u001B[38;5;66;03m# Crop image and mask tensors\u001B[39;00m\n\u001B[0;32m---> 89\u001B[0m data \u001B[38;5;241m=\u001B[39m data[\u001B[43mstart\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m:end[\u001B[38;5;241m0\u001B[39m], start[\u001B[38;5;241m1\u001B[39m]:end[\u001B[38;5;241m1\u001B[39m], start[\u001B[38;5;241m2\u001B[39m]:end[\u001B[38;5;241m2\u001B[39m]]\n\u001B[1;32m     90\u001B[0m mask \u001B[38;5;241m=\u001B[39m mask[start[\u001B[38;5;241m0\u001B[39m]:end[\u001B[38;5;241m0\u001B[39m], start[\u001B[38;5;241m1\u001B[39m]:end[\u001B[38;5;241m1\u001B[39m], start[\u001B[38;5;241m2\u001B[39m]:end[\u001B[38;5;241m2\u001B[39m]]\n\u001B[1;32m     92\u001B[0m data_tensor \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(data)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n",
      "\u001B[0;31mIndexError\u001B[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "def crop_to_largest_tumor(tensor, crop_size):\n",
    "    # Get tumor dimensions\n",
    "    tumor_size = np.sum(tensor > 0, axis=(1, 2, 3))\n",
    "    max_tumor_size = np.max(tumor_size)\n",
    "\n",
    "    # Calculate cropping indices\n",
    "    start = (tumor_size - max_tumor_size) // 2\n",
    "    end = start + max_tumor_size\n",
    "\n",
    "    # Perform cropping\n",
    "    cropped_tensor = tensor[:, start:end, start:end, start:end]\n",
    "\n",
    "    return cropped_tensor\n",
    "class BrainSegmentationDataset(Dataset):\n",
    "    def __init__(self, img_dir, labels_file, transform=None, target_transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_labels = pd.read_excel(labels_file)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = self.split_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the mask directly\n",
    "        seg_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0], 'seg.nii.gz')\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0], 't1_gd.nii.gz')\n",
    "        data_1 = nib.load(img_path)\n",
    "        data = data_1.get_fdata()\n",
    "        mask_1 = nib.load(seg_path)\n",
    "        mask = mask_1.get_fdata()\n",
    "        label = self.y_train[idx]\n",
    "\n",
    "        data = torch.from_numpy(data).unsqueeze(0)\n",
    "        mask = torch.from_numpy(mask).unsqueeze(0)\n",
    "        label = torch.tensor(label).long()\n",
    "\n",
    "        return mask, data, label\n",
    "\n",
    "    def split_data(self):\n",
    "        X = self.img_labels.iloc[:, 0].values\n",
    "        y = self.img_labels.iloc[:, 2].values\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)\n",
    "\n",
    "        X_train = self.process_images(X_train)\n",
    "        X_test = self.process_images(X_test)\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def process_images(self, image_names):\n",
    "        image_tensors = []\n",
    "\n",
    "        for image_name in image_names:\n",
    "            # Load the mask directly\n",
    "            seg_path = os.path.join(self.img_dir, image_name, 'seg.nii.gz')\n",
    "            img_path = os.path.join(self.img_dir, image_name, 't1_gd.nii.gz')\n",
    "            data_1 = nib.load(img_path)\n",
    "            data = data_1.get_fdata()\n",
    "            mask_1 = nib.load(seg_path)\n",
    "            mask = mask_1.get_fdata()\n",
    "\n",
    "            data_tensor = torch.from_numpy(data).unsqueeze(0)\n",
    "            mask_tensor = torch.from_numpy(mask).unsqueeze(0)\n",
    "\n",
    "            image_tensors.append((data_tensor, mask_tensor))\n",
    "\n",
    "        return image_tensors\n",
    "\n",
    "            tumor_size = np.sum(mask > 0, axis=(0, 1, 2))\n",
    "            print(tumor_size)\n",
    "            max_tumor_size = np.max(tumor_size)\n",
    "            print(max_tumor_size)\n",
    "\n",
    "            # Calculate cropping indices\n",
    "            start = (tumor_size - max_tumor_size) // 2\n",
    "            print(start)\n",
    "            end = start + max_tumor_size\n",
    "\n",
    "            # Crop image and mask tensors\n",
    "            data = data[start[0]:end[0], start[1]:end[1], start[2]:end[2]]\n",
    "            mask = mask[start[0]:end[0], start[1]:end[1], start[2]:end[2]]\n",
    "\n",
    "            data_tensor = torch.from_numpy(data).unsqueeze(0)\n",
    "            mask_tensor = torch.from_numpy(mask).unsqueeze(0)\n",
    "\n",
    "            image_tensors.append((data_tensor, mask_tensor))\n",
    "\n",
    "        return image_tensors\n",
    "\n",
    "img_dir = '/data/projects/TMOR/data/Deeplearning/'\n",
    "labels_file = 'labels_DL.xlsx'\n",
    "transform = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])\n",
    "\n",
    "dataset = BrainSegmentationDataset(img_dir, labels_file, transform=transform)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T20:46:34.373136Z",
     "end_time": "2023-06-19T20:47:15.555267Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## lets divide the tumors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/projects/TMOR/data/Deeplearning/Oslo02/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo02/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo03/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo03/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo04/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo04/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Stan_230/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Stan_230/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo06/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo06/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo07/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo07/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo08/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo08/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo09/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo09/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo10/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo10/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo11/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo11/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo12/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo12/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo13/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo13/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo14/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo14/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo17/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo17/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo18/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo18/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo19/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo19/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo20/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo20/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo21/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo21/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo22/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo22/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo23/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo23/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo24/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo24/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo25/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo25/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo26/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo26/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo28/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo28/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo29/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo29/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo30/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo30/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo31/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo31/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo32/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo32/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo33/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo33/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo34/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo34/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo35/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo35/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo36/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo36/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo37/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo37/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo38/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo38/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo39/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo39/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo40/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo40/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo41/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo41/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo42/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo42/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo43/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo43/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo44/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo44/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo45/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo45/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo46/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo46/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo47/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo47/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo48/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo48/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo49/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo49/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo50/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo50/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo51/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo51/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo52/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo52/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo53/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo53/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo54/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo54/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo55/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo55/t1_gd.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo56/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo56/t1_gd.nii.gz\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[104], line 17\u001B[0m\n\u001B[1;32m     15\u001B[0m mask \u001B[38;5;241m=\u001B[39m nib\u001B[38;5;241m.\u001B[39mload(seg)\n\u001B[1;32m     16\u001B[0m vol_mask \u001B[38;5;241m=\u001B[39m mask\u001B[38;5;241m.\u001B[39mget_fdata()\n\u001B[0;32m---> 17\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m label \u001B[38;5;129;01min\u001B[39;00m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munique\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvol_mask\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m label \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     19\u001B[0m         label_indices \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margwhere(vol_mask \u001B[38;5;241m==\u001B[39m label)\n",
      "File \u001B[0;32m<__array_function__ internals>:200\u001B[0m, in \u001B[0;36munique\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/site-packages/numpy/lib/arraysetops.py:274\u001B[0m, in \u001B[0;36munique\u001B[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001B[0m\n\u001B[1;32m    272\u001B[0m ar \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masanyarray(ar)\n\u001B[1;32m    273\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m axis \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 274\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43m_unique1d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mar\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_inverse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_counts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m    275\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mequal_nan\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mequal_nan\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    276\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _unpack_tuple(ret)\n\u001B[1;32m    278\u001B[0m \u001B[38;5;66;03m# axis was specified and not None\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/site-packages/numpy/lib/arraysetops.py:336\u001B[0m, in \u001B[0;36m_unique1d\u001B[0;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001B[0m\n\u001B[1;32m    334\u001B[0m     aux \u001B[38;5;241m=\u001B[39m ar[perm]\n\u001B[1;32m    335\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 336\u001B[0m     \u001B[43mar\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msort\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    337\u001B[0m     aux \u001B[38;5;241m=\u001B[39m ar\n\u001B[1;32m    338\u001B[0m mask \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mempty(aux\u001B[38;5;241m.\u001B[39mshape, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mbool_)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "seg_dir = '/data/projects/TMOR/data/Deeplearning/'\n",
    "#indices = []\n",
    "for subj_id in os.listdir(seg_dir):\n",
    "    #extrasct the correct id from the subjects\n",
    "    # Define paths to the segmentation/mask and regular t1 postprocessed files for this subject\n",
    "    seg_glob = os.path.join(seg_dir, f'{subj_id}', f'seg.nii.gz')\n",
    "    reg_glob = os.path.join(seg_dir, f'{subj_id}', f't1_gd.nii.gz')\n",
    "    seg_paths = glob.glob(seg_glob)\n",
    "    reg_paths = glob.glob(reg_glob)\n",
    "    # Loop through all matching segmentation and registration files\n",
    "    for seg, reg in zip(seg_paths, reg_paths):\n",
    "        if os.path.exists(seg) and os.path.exists(reg):\n",
    "            print(seg)\n",
    "            print(reg)\n",
    "            mask = nib.load(seg)\n",
    "            vol_mask = mask.get_fdata()\n",
    "            for label in np.unique(vol_mask):\n",
    "                if label != 0:\n",
    "                    label_indices = np.argwhere(vol_mask == label)\n",
    "                    if len(label_indices) > 125:\n",
    "                        indices = label_indices\n",
    "                        X_min, X_max = np.min(indices[:, 0]), np.max(indices[:, 0])\n",
    "                        Y_min, Y_max = np.min(indices[:, 1]), np.max(indices[:, 1])\n",
    "                        Z_min, Z_max = np.min(indices[:, 2]), np.max(indices[:, 2])\n",
    "                        seg_partitioned = vol_mask[X_min:X_max, Y_min:Y_max, Z_min:Z_max]\n",
    "\n",
    "                        output_dir = os.path.join(seg_dir, 'final', f'{subj_id}')\n",
    "                        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "                        # Save the seg_partitioned into a file\n",
    "                        output_path = os.path.join(output_dir, 'seg_seg_' + f'{str(label)}' +'.nii.gz')\n",
    "                        nib.save(nib.Nifti1Image(seg_partitioned, mask.affine), output_path)\n",
    "\n",
    "#for label_indices in indices:\n",
    "        # Do something with the label indices\n",
    "        #print(f\"Label: {label_indices[0, 3]}, Length: {len(label_indices)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-21T14:17:39.901419Z",
     "end_time": "2023-06-21T14:18:37.433023Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "D_x_max, D_y_max, D_z_max = 0, 0, 0\n",
    "seg_dir = '/data/projects/TMOR/data/Deeplearning/'\n",
    "#indices = []\n",
    "for subj_id in os.listdir(seg_dir):\n",
    "    #extrasct the correct id from the subjects\n",
    "    # Define paths to the segmentation and regular t1 postprocessed files for this subject\n",
    "    seg_glob = os.path.join(seg_dir, f'{subj_id}', f'seg.nii.gz')\n",
    "    reg_glob = os.path.join(seg_dir, f'{subj_id}', f't1_gd.nii.gz')\n",
    "    seg_paths = glob.glob(seg_glob)\n",
    "    reg_paths = glob.glob(reg_glob)\n",
    "    # Loop through all matching segmentation and registration files\n",
    "    for seg, reg in zip(seg_paths, reg_paths):\n",
    "        if os.path.exists(seg) and os.path.exists(reg):\n",
    "            mask = nib.load(seg)\n",
    "            vol_mask = mask.get_fdata()\n",
    "            for label in np.unique(vol_mask):\n",
    "                if label != 0:\n",
    "                    label_indices = np.argwhere(vol_mask == label)\n",
    "                    if len(label_indices) > 125:\n",
    "                        indices = label_indices\n",
    "                        X_min, X_max = np.min(indices[:, 0]), np.max(indices[:, 0])\n",
    "                        Y_min, Y_max = np.min(indices[:, 1]), np.max(indices[:, 1])\n",
    "                        Z_min, Z_max = np.min(indices[:, 2]), np.max(indices[:, 2])\n",
    "                        #we need the differences now boi\n",
    "                        D_x = X_max - X_min\n",
    "                        D_y = Y_max - Y_min\n",
    "                        D_z = Z_max - Z_min\n",
    "                        if D_x_max < D_x:\n",
    "                        \tD_x_max = D_x\n",
    "                        if D_y_max < D_y:\n",
    "                        \tD_y_max = D_y\n",
    "                        if D_z_max < D_z:\n",
    "                        \tD_z_max = D_z"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T15:32:35.365587Z",
     "end_time": "2023-07-06T15:33:56.863759Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "drop_dir = '/data/projects/TMOR/data/'\n",
    "seg_dir = '/data/projects/TMOR/data/Deeplearning/'\n",
    "#indices = []\n",
    "for subj_id in os.listdir(seg_dir):\n",
    "    #extrasct the correct id from the subjects\n",
    "    # Define paths to the segmentation and regular t1 postprocessed files for this subject\n",
    "    seg_glob = os.path.join(seg_dir, f'{subj_id}', f'seg.nii.gz')\n",
    "    reg_glob = os.path.join(seg_dir, f'{subj_id}', f't1_gd.nii.gz')\n",
    "    reg2_glob = os.path.join(seg_dir, f'{subj_id}', f't1_pre.nii.gz')\n",
    "    seg_paths = glob.glob(seg_glob)\n",
    "    reg_paths = glob.glob(reg_glob)\n",
    "    reg2_paths = glob.glob(reg2_glob)\n",
    "    # Loop through all matching segmentation and registration files\n",
    "    for seg, reg, reg2 in zip(seg_paths, reg_paths, reg2_paths):\n",
    "        if os.path.exists(seg) and os.path.exists(reg) and os.path.exists(reg2):\n",
    "            mask = nib.load(seg)\n",
    "            vol_mask = mask.get_fdata()\n",
    "            t1_gd = nib.load(reg)\n",
    "            vol_t1_gd = t1_gd.get_fdata()\n",
    "            t1_pre = nib.load(reg2)\n",
    "            vol_t1_pre = t1_pre.get_fdata()\n",
    "            for label in np.unique(vol_mask):\n",
    "                if label != 0:\n",
    "                    label_indices = np.argwhere(vol_mask == label)\n",
    "                    if len(label_indices) > 125:\n",
    "                        indices = label_indices\n",
    "                        X_min, X_max = np.min(indices[:, 0]), np.max(indices[:, 0])\n",
    "                        Y_min, Y_max = np.min(indices[:, 1]), np.max(indices[:, 1])\n",
    "                        Z_min, Z_max = np.min(indices[:, 2]), np.max(indices[:, 2])\n",
    "\n",
    "                        X_avg = (X_min + X_max) // 2\n",
    "                        Y_avg = (Y_min + Y_max) // 2\n",
    "                        Z_avg = (Z_min + Z_max) // 2\n",
    "                        # Pad the indices with zeros\n",
    "                        X_low = max(X_avg - (D_x_max // 2), 0)\n",
    "                        X_high = min(X_avg + (D_x_max // 2), vol_mask.shape[0])\n",
    "                        Y_low = max(Y_avg - (D_y_max // 2), 0)\n",
    "                        Y_high = min(Y_avg + (D_y_max // 2), vol_mask.shape[1])\n",
    "                        Z_low = max(Z_avg - (D_z_max // 2), 0)\n",
    "                        Z_high = min(Z_avg + (D_z_max // 2), vol_mask.shape[2])\n",
    "\n",
    "                        pene = vol_mask[X_low:X_high, Y_low:Y_high, Z_low:Z_high]\n",
    "                        t1_desp = vol_t1_gd[X_low:X_high, Y_low:Y_high, Z_low:Z_high]\n",
    "                        t1_antes = vol_t1_pre[X_low:X_high, Y_low:Y_high, Z_low:Z_high]\n",
    "\n",
    "                        ## define the pads\n",
    "                        pad_width = [\n",
    "                            (0, max(D_x_max - pene.shape[0], 0)),\n",
    "                            (0, max(D_y_max - pene.shape[1], 0)),\n",
    "                            (0, max(D_z_max - pene.shape[2], 0))\n",
    "                        ]\n",
    "                        # define the partitions\n",
    "                        seg_partitioned = np.pad(pene, pad_width, mode='constant', constant_values=0)\n",
    "                        t1_gd_partitioned = np.pad(t1_desp, pad_width, mode='constant', constant_values=0)\n",
    "                        t1_pre_partitioned = np.pad(t1_antes, pad_width, mode='constant', constant_values=0)\n",
    "                        #\n",
    "                        output_dir = os.path.join(drop_dir, 'VeryFinal', subj_id, str(int(label)))\n",
    "                        os.makedirs(output_dir, exist_ok=True)\n",
    "                        # tumors segemented of the segmentation of mask\n",
    "                        output_path_1 = os.path.join(output_dir, 'seg.nii.gz')\n",
    "                        nib.save(nib.Nifti1Image(seg_partitioned, mask.affine), output_path_1)\n",
    "                        # tumors segmented of the t1_gd\n",
    "                        output_path_2 = os.path.join(output_dir, 't1_gd.nii.gz')\n",
    "                        nib.save(nib.Nifti1Image(t1_gd_partitioned, t1_gd.affine), output_path_2)\n",
    "                        # tumors segemented if t1_pre\n",
    "                        output_path_3 = os.path.join(output_dir, 't1_pre.nii.gz')\n",
    "                        nib.save(nib.Nifti1Image(t1_pre_partitioned, t1_pre.affine), output_path_3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T15:34:54.740170Z",
     "end_time": "2023-07-06T15:37:02.430156Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999997671694\n"
     ]
    }
   ],
   "source": [
    "seg = f'/data/projects/TMOR/data_v2/Oslo/Subject66/seg_66.nii.gz'\n",
    "\n",
    "mask = nib.load(seg)\n",
    "vol_mask = mask.get_fdata()\n",
    "np.unique(vol_mask)\n",
    "for label in np.unique(vol_mask):\n",
    "    if label != 0:\n",
    "        label_indices = np.argwhere(vol_mask == label)\n",
    "        if len(label_indices) > 125:\n",
    "            print(label)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T15:49:55.629350Z",
     "end_time": "2023-07-06T15:50:21.938710Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "data": {
      "text/plain": "label                     1.000000\nshape_Elongation          0.860610\nshape_Flatness            0.856119\nshape_LeastAxisLength     7.661783\nshape_MajorAxisLength     8.949440\n                           ...    \nAge                      78.000000\nnumber_annotations        1.000000\nF                         1.000000\nM                         0.000000\nlabels                    0.000000\nName: Oslo66, Length: 113, dtype: float64"
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.loc['Oslo66']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T15:51:55.203964Z",
     "end_time": "2023-07-06T15:51:56.062821Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "2.0\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "seg = f'/data/projects/TMOR/data_v2/Oslo/Subject65/seg_65.nii.gz'\n",
    "\n",
    "mask = nib.load(seg)\n",
    "vol_mask = mask.get_fdata()\n",
    "np.unique(vol_mask)\n",
    "for label in np.unique(vol_mask):\n",
    "    if label != 0:\n",
    "        label_indices = np.argwhere(vol_mask == label)\n",
    "        if len(label_indices) > 125:\n",
    "            print(label)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T15:50:34.333503Z",
     "end_time": "2023-07-06T15:50:34.701584Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "data": {
      "text/plain": "        label  shape_Elongation  shape_Flatness  shape_LeastAxisLength   \nsubjid                                                                   \nOslo65      1          0.855166        0.800732              20.767148  \\\nOslo65      2          0.842210        0.762170               6.820878   \nOslo65      3          0.973216        0.814385              21.189772   \n\n        shape_MajorAxisLength  shape_Maximum2DDiameterColumn   \nsubjid                                                         \nOslo65              25.935220                      28.301943  \\\nOslo65               8.949284                       9.486833   \nOslo65              26.019365                      34.365681   \n\n        shape_Maximum2DDiameterRow  shape_Maximum2DDiameterSlice   \nsubjid                                                             \nOslo65                   26.683328                     29.832868  \\\nOslo65                   10.000000                     10.295630   \nOslo65                   34.000000                     37.589892   \n\n        shape_Maximum3DDiameter  shape_MeshVolume  ...  ngtdm_Busyness   \nsubjid                                             ...                   \nOslo65                31.048349       6924.333333  ...      204.346342  \\\nOslo65                10.677078        317.291667  ...        1.579231   \nOslo65                38.716921       8639.791667  ...      255.052021   \n\n        ngtdm_Coarseness  ngtdm_Complexity  ngtdm_Contrast  ngtdm_Strength   \nsubjid                                                                       \nOslo65          0.001758          0.163391        0.027139        0.001712  \\\nOslo65          0.164355          0.037328        0.000915        0.160495   \nOslo65          0.001400          0.164962        0.026721        0.001384   \n\n        Age  number_annotations  F  M  labels  \nsubjid                                         \nOslo65   65                   1  1  0       1  \nOslo65   65                   1  1  0       1  \nOslo65   65                   1  1  0       1  \n\n[3 rows x 113 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>shape_Elongation</th>\n      <th>shape_Flatness</th>\n      <th>shape_LeastAxisLength</th>\n      <th>shape_MajorAxisLength</th>\n      <th>shape_Maximum2DDiameterColumn</th>\n      <th>shape_Maximum2DDiameterRow</th>\n      <th>shape_Maximum2DDiameterSlice</th>\n      <th>shape_Maximum3DDiameter</th>\n      <th>shape_MeshVolume</th>\n      <th>...</th>\n      <th>ngtdm_Busyness</th>\n      <th>ngtdm_Coarseness</th>\n      <th>ngtdm_Complexity</th>\n      <th>ngtdm_Contrast</th>\n      <th>ngtdm_Strength</th>\n      <th>Age</th>\n      <th>number_annotations</th>\n      <th>F</th>\n      <th>M</th>\n      <th>labels</th>\n    </tr>\n    <tr>\n      <th>subjid</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Oslo65</th>\n      <td>1</td>\n      <td>0.855166</td>\n      <td>0.800732</td>\n      <td>20.767148</td>\n      <td>25.935220</td>\n      <td>28.301943</td>\n      <td>26.683328</td>\n      <td>29.832868</td>\n      <td>31.048349</td>\n      <td>6924.333333</td>\n      <td>...</td>\n      <td>204.346342</td>\n      <td>0.001758</td>\n      <td>0.163391</td>\n      <td>0.027139</td>\n      <td>0.001712</td>\n      <td>65</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>Oslo65</th>\n      <td>2</td>\n      <td>0.842210</td>\n      <td>0.762170</td>\n      <td>6.820878</td>\n      <td>8.949284</td>\n      <td>9.486833</td>\n      <td>10.000000</td>\n      <td>10.295630</td>\n      <td>10.677078</td>\n      <td>317.291667</td>\n      <td>...</td>\n      <td>1.579231</td>\n      <td>0.164355</td>\n      <td>0.037328</td>\n      <td>0.000915</td>\n      <td>0.160495</td>\n      <td>65</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>Oslo65</th>\n      <td>3</td>\n      <td>0.973216</td>\n      <td>0.814385</td>\n      <td>21.189772</td>\n      <td>26.019365</td>\n      <td>34.365681</td>\n      <td>34.000000</td>\n      <td>37.589892</td>\n      <td>38.716921</td>\n      <td>8639.791667</td>\n      <td>...</td>\n      <td>255.052021</td>\n      <td>0.001400</td>\n      <td>0.164962</td>\n      <td>0.026721</td>\n      <td>0.001384</td>\n      <td>65</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 113 columns</p>\n</div>"
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.loc['Oslo65']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T15:51:21.231183Z",
     "end_time": "2023-07-06T15:51:21.271413Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "data": {
      "text/plain": "          label  shape_Elongation  shape_Flatness  shape_LeastAxisLength   \nsubjid                                                                     \nStan_098      1          0.738644        0.686814              12.638130  \\\nStan_098      3          0.951212        0.816792               5.247428   \n\n          shape_MajorAxisLength  shape_Maximum2DDiameterColumn   \nsubjid                                                           \nStan_098              18.401099                      20.880613  \\\nStan_098               6.424437                       7.280110   \n\n          shape_Maximum2DDiameterRow  shape_Maximum2DDiameterSlice   \nsubjid                                                               \nStan_098                   20.880613                     17.088007  \\\nStan_098                    7.615773                      7.810250   \n\n          shape_Maximum3DDiameter  shape_MeshVolume  ...  ngtdm_Busyness   \nsubjid                                               ...                   \nStan_098                21.908902       2199.291667  ...       30.766956  \\\nStan_098                 8.124038        120.375000  ...        0.309877   \n\n          ngtdm_Coarseness  ngtdm_Complexity  ngtdm_Contrast  ngtdm_Strength   \nsubjid                                                                         \nStan_098          0.009691          0.093001        0.008930        0.009700  \\\nStan_098          0.835931          0.018547        0.000678        0.519532   \n\n          Age  number_annotations  F  M  labels  \nsubjid                                           \nStan_098   61                   1  1  0       1  \nStan_098   61                   1  1  0       1  \n\n[2 rows x 113 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>shape_Elongation</th>\n      <th>shape_Flatness</th>\n      <th>shape_LeastAxisLength</th>\n      <th>shape_MajorAxisLength</th>\n      <th>shape_Maximum2DDiameterColumn</th>\n      <th>shape_Maximum2DDiameterRow</th>\n      <th>shape_Maximum2DDiameterSlice</th>\n      <th>shape_Maximum3DDiameter</th>\n      <th>shape_MeshVolume</th>\n      <th>...</th>\n      <th>ngtdm_Busyness</th>\n      <th>ngtdm_Coarseness</th>\n      <th>ngtdm_Complexity</th>\n      <th>ngtdm_Contrast</th>\n      <th>ngtdm_Strength</th>\n      <th>Age</th>\n      <th>number_annotations</th>\n      <th>F</th>\n      <th>M</th>\n      <th>labels</th>\n    </tr>\n    <tr>\n      <th>subjid</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Stan_098</th>\n      <td>1</td>\n      <td>0.738644</td>\n      <td>0.686814</td>\n      <td>12.638130</td>\n      <td>18.401099</td>\n      <td>20.880613</td>\n      <td>20.880613</td>\n      <td>17.088007</td>\n      <td>21.908902</td>\n      <td>2199.291667</td>\n      <td>...</td>\n      <td>30.766956</td>\n      <td>0.009691</td>\n      <td>0.093001</td>\n      <td>0.008930</td>\n      <td>0.009700</td>\n      <td>61</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>Stan_098</th>\n      <td>3</td>\n      <td>0.951212</td>\n      <td>0.816792</td>\n      <td>5.247428</td>\n      <td>6.424437</td>\n      <td>7.280110</td>\n      <td>7.615773</td>\n      <td>7.810250</td>\n      <td>8.124038</td>\n      <td>120.375000</td>\n      <td>...</td>\n      <td>0.309877</td>\n      <td>0.835931</td>\n      <td>0.018547</td>\n      <td>0.000678</td>\n      <td>0.519532</td>\n      <td>61</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows × 113 columns</p>\n</div>"
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.loc['Stan_098']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T15:43:51.914709Z",
     "end_time": "2023-07-06T15:43:51.964633Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBJECT Oslo02 WITH 5.0 WAS PADDED\n",
      "SUBJECT Oslo03 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo03 WITH 3.0 WAS PADDED\n",
      "SUBJECT Oslo04 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_230 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo06 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo07 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo08 WITH 3.0 WAS PADDED\n",
      "SUBJECT Oslo09 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo09 WITH 2.0 WAS PADDED\n",
      "SUBJECT Oslo10 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo11 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo12 WITH 2.0 WAS PADDED\n",
      "SUBJECT Oslo13 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo14 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo17 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo18 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo18 WITH 2.0 WAS PADDED\n",
      "SUBJECT Oslo18 WITH 3.0 WAS PADDED\n",
      "SUBJECT Oslo19 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo20 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo20 WITH 2.0 WAS PADDED\n",
      "SUBJECT Oslo21 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo22 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo23 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo24 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo24 WITH 2.0 WAS PADDED\n",
      "SUBJECT Oslo25 WITH 11.0 WAS PADDED\n",
      "SUBJECT Oslo26 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo26 WITH 2.0 WAS PADDED\n",
      "SUBJECT Oslo28 WITH 4.0 WAS PADDED\n",
      "SUBJECT Oslo29 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo30 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo30 WITH 2.0 WAS PADDED\n",
      "SUBJECT Oslo31 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo31 WITH 2.0 WAS PADDED\n",
      "SUBJECT Oslo32 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo32 WITH 3.0 WAS PADDED\n",
      "SUBJECT Oslo33 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo33 WITH 2.0 WAS PADDED\n",
      "SUBJECT Oslo34 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo35 WITH 2.0 WAS PADDED\n",
      "SUBJECT Oslo36 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo37 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo37 WITH 2.0 WAS PADDED\n",
      "SUBJECT Oslo38 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo38 WITH 3.0 WAS PADDED\n",
      "SUBJECT Oslo39 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo40 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo41 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo41 WITH 2.0 WAS PADDED\n",
      "SUBJECT Oslo42 WITH 3.0 WAS PADDED\n",
      "SUBJECT Oslo43 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo43 WITH 3.0 WAS PADDED\n",
      "SUBJECT Oslo43 WITH 4.0 WAS PADDED\n",
      "SUBJECT Oslo44 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo45 WITH 2.0 WAS PADDED\n",
      "SUBJECT Oslo45 WITH 3.0 WAS PADDED\n",
      "SUBJECT Oslo46 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo47 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo48 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo48 WITH 3.0 WAS PADDED\n",
      "SUBJECT Oslo49 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo49 WITH 2.0 WAS PADDED\n",
      "SUBJECT Oslo49 WITH 3.0 WAS PADDED\n",
      "SUBJECT Oslo50 WITH 2.0 WAS PADDED\n",
      "SUBJECT Oslo51 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo52 WITH 3.0 WAS PADDED\n",
      "SUBJECT Oslo52 WITH 4.0 WAS PADDED\n",
      "SUBJECT Oslo53 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo53 WITH 2.0 WAS PADDED\n",
      "SUBJECT Oslo54 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo55 WITH 2.0 WAS PADDED\n",
      "SUBJECT Oslo56 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_225 WITH 4.0 WAS PADDED\n",
      "SUBJECT Stan_225 WITH 7.0 WAS PADDED\n",
      "SUBJECT Stan_225 WITH 9.0 WAS PADDED\n",
      "SUBJECT Oslo59 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo60 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo61 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo62 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo63 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo63 WITH 2.0 WAS PADDED\n",
      "SUBJECT Oslo63 WITH 3.0 WAS PADDED\n",
      "SUBJECT Oslo63 WITH 4.0 WAS PADDED\n",
      "SUBJECT Oslo65 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo65 WITH 2.0 WAS PADDED\n",
      "SUBJECT Oslo65 WITH 3.0 WAS PADDED\n",
      "SUBJECT Stan_005 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_005 WITH 2.0 WAS PADDED\n",
      "SUBJECT Stan_005 WITH 3.0 WAS PADDED\n",
      "SUBJECT Stan_009 WITH 8.0 WAS PADDED\n",
      "SUBJECT Stan_009 WITH 13.0 WAS PADDED\n",
      "SUBJECT Stan_010 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_010 WITH 3.0 WAS PADDED\n",
      "SUBJECT Stan_014 WITH 5.0 WAS PADDED\n",
      "SUBJECT Stan_014 WITH 6.0 WAS PADDED\n",
      "SUBJECT Stan_014 WITH 7.0 WAS PADDED\n",
      "SUBJECT Stan_014 WITH 9.0 WAS PADDED\n",
      "SUBJECT Stan_014 WITH 10.0 WAS PADDED\n",
      "SUBJECT Stan_014 WITH 11.0 WAS PADDED\n",
      "SUBJECT Stan_016 WITH 2.0 WAS PADDED\n",
      "SUBJECT Stan_016 WITH 3.0 WAS PADDED\n",
      "SUBJECT Stan_016 WITH 5.0 WAS PADDED\n",
      "SUBJECT Stan_016 WITH 6.0 WAS PADDED\n",
      "SUBJECT Stan_019 WITH 2.0 WAS PADDED\n",
      "SUBJECT Stan_019 WITH 7.0 WAS PADDED\n",
      "SUBJECT Stan_019 WITH 12.0 WAS PADDED\n",
      "SUBJECT Stan_021 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_021 WITH 6.0 WAS PADDED\n",
      "SUBJECT Stan_021 WITH 16.0 WAS PADDED\n",
      "SUBJECT Stan_024 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_024 WITH 46.0 WAS PADDED\n",
      "SUBJECT Stan_024 WITH 65.0 WAS PADDED\n",
      "SUBJECT Stan_026 WITH 4.0 WAS PADDED\n",
      "SUBJECT Stan_026 WITH 11.0 WAS PADDED\n",
      "SUBJECT Stan_028 WITH 4.0 WAS PADDED\n",
      "SUBJECT Stan_028 WITH 7.0 WAS PADDED\n",
      "SUBJECT Stan_032 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_032 WITH 2.0 WAS PADDED\n",
      "SUBJECT Stan_032 WITH 5.0 WAS PADDED\n",
      "SUBJECT Stan_033 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_036 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_037 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_037 WITH 5.0 WAS PADDED\n",
      "SUBJECT Stan_038 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_039 WITH 4.0 WAS PADDED\n",
      "SUBJECT Stan_039 WITH 6.0 WAS PADDED\n",
      "SUBJECT Stan_039 WITH 7.0 WAS PADDED\n",
      "SUBJECT Stan_041 WITH 2.0 WAS PADDED\n",
      "SUBJECT Stan_041 WITH 3.0 WAS PADDED\n",
      "SUBJECT Stan_045 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_045 WITH 2.0 WAS PADDED\n",
      "SUBJECT Stan_047 WITH 6.0 WAS PADDED\n",
      "SUBJECT Stan_047 WITH 16.0 WAS PADDED\n",
      "SUBJECT Stan_049 WITH 2.0 WAS PADDED\n",
      "SUBJECT Stan_049 WITH 3.0 WAS PADDED\n",
      "SUBJECT Stan_049 WITH 5.0 WAS PADDED\n",
      "SUBJECT Stan_049 WITH 6.0 WAS PADDED\n",
      "SUBJECT Stan_049 WITH 9.0 WAS PADDED\n",
      "SUBJECT Stan_049 WITH 10.0 WAS PADDED\n",
      "SUBJECT Stan_049 WITH 11.0 WAS PADDED\n",
      "SUBJECT Stan_051 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_051 WITH 8.0 WAS PADDED\n",
      "SUBJECT Stan_051 WITH 9.0 WAS PADDED\n",
      "SUBJECT Stan_052 WITH 23.0 WAS PADDED\n",
      "SUBJECT Stan_052 WITH 74.0 WAS PADDED\n",
      "SUBJECT Stan_052 WITH 157.0 WAS PADDED\n",
      "SUBJECT Stan_053 WITH 14.0 WAS PADDED\n",
      "SUBJECT Stan_053 WITH 16.0 WAS PADDED\n",
      "SUBJECT Stan_053 WITH 20.0 WAS PADDED\n",
      "SUBJECT Stan_054 WITH 25.0 WAS PADDED\n",
      "SUBJECT Stan_055 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_055 WITH 2.0 WAS PADDED\n",
      "SUBJECT Stan_055 WITH 3.0 WAS PADDED\n",
      "SUBJECT Stan_055 WITH 5.0 WAS PADDED\n",
      "SUBJECT Stan_055 WITH 6.0 WAS PADDED\n",
      "SUBJECT Stan_055 WITH 7.0 WAS PADDED\n",
      "SUBJECT Stan_055 WITH 8.0 WAS PADDED\n",
      "SUBJECT Stan_055 WITH 18.0 WAS PADDED\n",
      "SUBJECT Stan_058 WITH 5.0 WAS PADDED\n",
      "SUBJECT Stan_058 WITH 6.0 WAS PADDED\n",
      "SUBJECT Stan_058 WITH 8.0 WAS PADDED\n",
      "SUBJECT Stan_058 WITH 11.0 WAS PADDED\n",
      "SUBJECT Stan_058 WITH 13.0 WAS PADDED\n",
      "SUBJECT Stan_058 WITH 15.0 WAS PADDED\n",
      "SUBJECT Stan_058 WITH 17.0 WAS PADDED\n",
      "SUBJECT Stan_059 WITH 7.0 WAS PADDED\n",
      "SUBJECT Stan_059 WITH 15.0 WAS PADDED\n",
      "SUBJECT Stan_064 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_064 WITH 4.0 WAS PADDED\n",
      "SUBJECT Stan_064 WITH 8.0 WAS PADDED\n",
      "SUBJECT Stan_064 WITH 9.0 WAS PADDED\n",
      "SUBJECT Stan_064 WITH 25.0 WAS PADDED\n",
      "SUBJECT Stan_064 WITH 27.0 WAS PADDED\n",
      "SUBJECT Stan_064 WITH 31.0 WAS PADDED\n",
      "SUBJECT Stan_064 WITH 43.0 WAS PADDED\n",
      "SUBJECT Stan_065 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_065 WITH 2.0 WAS PADDED\n",
      "SUBJECT Stan_065 WITH 3.0 WAS PADDED\n",
      "SUBJECT Stan_065 WITH 4.0 WAS PADDED\n",
      "SUBJECT Stan_065 WITH 5.0 WAS PADDED\n",
      "SUBJECT Stan_065 WITH 6.0 WAS PADDED\n",
      "SUBJECT Stan_066 WITH 4.0 WAS PADDED\n",
      "SUBJECT Stan_066 WITH 18.0 WAS PADDED\n",
      "SUBJECT Stan_066 WITH 26.0 WAS PADDED\n",
      "SUBJECT Stan_066 WITH 27.0 WAS PADDED\n",
      "SUBJECT Stan_066 WITH 28.0 WAS PADDED\n",
      "SUBJECT Stan_068 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_068 WITH 2.0 WAS PADDED\n",
      "SUBJECT Stan_068 WITH 3.0 WAS PADDED\n",
      "SUBJECT Stan_069 WITH 3.0 WAS PADDED\n",
      "SUBJECT Stan_069 WITH 5.0 WAS PADDED\n",
      "SUBJECT Stan_072 WITH 3.0 WAS PADDED\n",
      "SUBJECT Stan_074 WITH 3.0 WAS PADDED\n",
      "SUBJECT Stan_081 WITH 6.0 WAS PADDED\n",
      "SUBJECT Stan_087 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_087 WITH 2.0 WAS PADDED\n",
      "SUBJECT Stan_089 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_096 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_096 WITH 4.0 WAS PADDED\n",
      "SUBJECT Stan_096 WITH 6.0 WAS PADDED\n",
      "SUBJECT Stan_096 WITH 7.0 WAS PADDED\n",
      "SUBJECT Stan_096 WITH 8.0 WAS PADDED\n",
      "SUBJECT Stan_098 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_098 WITH 3.0 WAS PADDED\n",
      "SUBJECT Stan_099 WITH 3.0 WAS PADDED\n",
      "SUBJECT Stan_099 WITH 4.0 WAS PADDED\n",
      "SUBJECT Stan_099 WITH 6.0 WAS PADDED\n",
      "SUBJECT Stan_100 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_100 WITH 2.0 WAS PADDED\n",
      "SUBJECT Stan_101 WITH 4.0 WAS PADDED\n",
      "SUBJECT Stan_101 WITH 5.0 WAS PADDED\n",
      "SUBJECT Stan_102 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_107 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_107 WITH 10.0 WAS PADDED\n",
      "SUBJECT Stan_107 WITH 13.0 WAS PADDED\n",
      "SUBJECT Stan_107 WITH 14.0 WAS PADDED\n",
      "SUBJECT Stan_107 WITH 26.0 WAS PADDED\n",
      "SUBJECT Stan_107 WITH 28.0 WAS PADDED\n",
      "SUBJECT Stan_107 WITH 38.0 WAS PADDED\n",
      "SUBJECT Stan_107 WITH 42.0 WAS PADDED\n",
      "SUBJECT Stan_111 WITH 3.0 WAS PADDED\n",
      "SUBJECT Stan_111 WITH 5.0 WAS PADDED\n",
      "SUBJECT Stan_120 WITH 8.0 WAS PADDED\n",
      "SUBJECT Stan_120 WITH 19.0 WAS PADDED\n",
      "SUBJECT Stan_120 WITH 29.0 WAS PADDED\n",
      "SUBJECT Stan_120 WITH 30.0 WAS PADDED\n",
      "SUBJECT Stan_120 WITH 34.0 WAS PADDED\n",
      "SUBJECT Stan_120 WITH 39.0 WAS PADDED\n",
      "SUBJECT Stan_120 WITH 41.0 WAS PADDED\n",
      "SUBJECT Stan_120 WITH 48.0 WAS PADDED\n",
      "SUBJECT Stan_120 WITH 61.0 WAS PADDED\n",
      "SUBJECT Stan_120 WITH 94.0 WAS PADDED\n",
      "SUBJECT Stan_120 WITH 95.0 WAS PADDED\n",
      "SUBJECT Stan_120 WITH 105.0 WAS PADDED\n",
      "SUBJECT Stan_120 WITH 106.0 WAS PADDED\n",
      "SUBJECT Stan_120 WITH 145.0 WAS PADDED\n",
      "SUBJECT Stan_121 WITH 3.0 WAS PADDED\n",
      "SUBJECT Stan_121 WITH 4.0 WAS PADDED\n",
      "SUBJECT Stan_121 WITH 10.0 WAS PADDED\n",
      "SUBJECT Stan_121 WITH 21.0 WAS PADDED\n",
      "SUBJECT Stan_121 WITH 22.0 WAS PADDED\n",
      "SUBJECT Stan_121 WITH 28.0 WAS PADDED\n",
      "SUBJECT Stan_121 WITH 30.0 WAS PADDED\n",
      "SUBJECT Stan_121 WITH 39.0 WAS PADDED\n",
      "SUBJECT Stan_121 WITH 43.0 WAS PADDED\n",
      "SUBJECT Stan_121 WITH 48.0 WAS PADDED\n",
      "SUBJECT Stan_121 WITH 49.0 WAS PADDED\n",
      "SUBJECT Stan_121 WITH 54.0 WAS PADDED\n",
      "SUBJECT Stan_121 WITH 56.0 WAS PADDED\n",
      "SUBJECT Stan_121 WITH 70.0 WAS PADDED\n",
      "SUBJECT Stan_121 WITH 71.0 WAS PADDED\n",
      "SUBJECT Stan_121 WITH 72.0 WAS PADDED\n",
      "SUBJECT Stan_121 WITH 77.0 WAS PADDED\n",
      "SUBJECT Stan_121 WITH 92.0 WAS PADDED\n",
      "SUBJECT Stan_121 WITH 94.0 WAS PADDED\n",
      "SUBJECT Stan_121 WITH 97.0 WAS PADDED\n",
      "SUBJECT Stan_121 WITH 105.0 WAS PADDED\n",
      "SUBJECT Stan_121 WITH 107.0 WAS PADDED\n",
      "SUBJECT Stan_121 WITH 132.0 WAS PADDED\n",
      "SUBJECT Stan_121 WITH 134.0 WAS PADDED\n",
      "SUBJECT Stan_121 WITH 154.0 WAS PADDED\n",
      "SUBJECT Stan_122 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_123 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_123 WITH 4.0 WAS PADDED\n",
      "SUBJECT Stan_123 WITH 6.0 WAS PADDED\n",
      "SUBJECT Stan_123 WITH 8.0 WAS PADDED\n",
      "SUBJECT Stan_126 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_127 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_132 WITH 17.0 WAS PADDED\n",
      "SUBJECT Stan_132 WITH 20.0 WAS PADDED\n",
      "SUBJECT Stan_134 WITH 5.0 WAS PADDED\n",
      "SUBJECT Stan_134 WITH 46.0 WAS PADDED\n",
      "SUBJECT Stan_134 WITH 50.0 WAS PADDED\n",
      "SUBJECT Stan_134 WITH 53.0 WAS PADDED\n",
      "SUBJECT Stan_134 WITH 58.0 WAS PADDED\n",
      "SUBJECT Stan_134 WITH 83.0 WAS PADDED\n",
      "SUBJECT Stan_136 WITH 3.0 WAS PADDED\n",
      "SUBJECT Stan_136 WITH 13.0 WAS PADDED\n",
      "SUBJECT Stan_136 WITH 34.0 WAS PADDED\n",
      "SUBJECT Stan_136 WITH 47.0 WAS PADDED\n",
      "SUBJECT Stan_136 WITH 49.0 WAS PADDED\n",
      "SUBJECT Stan_136 WITH 53.0 WAS PADDED\n",
      "SUBJECT Stan_136 WITH 55.0 WAS PADDED\n",
      "SUBJECT Stan_142 WITH 4.0 WAS PADDED\n",
      "SUBJECT Stan_142 WITH 8.0 WAS PADDED\n",
      "SUBJECT Stan_144 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_144 WITH 2.0 WAS PADDED\n",
      "SUBJECT Stan_144 WITH 3.0 WAS PADDED\n",
      "SUBJECT Stan_144 WITH 4.0 WAS PADDED\n",
      "SUBJECT Stan_144 WITH 5.0 WAS PADDED\n",
      "SUBJECT Stan_144 WITH 6.0 WAS PADDED\n",
      "SUBJECT Stan_148 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_149 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_165 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_170 WITH 7.0 WAS PADDED\n",
      "SUBJECT Stan_170 WITH 9.0 WAS PADDED\n",
      "SUBJECT Stan_170 WITH 15.0 WAS PADDED\n",
      "SUBJECT Stan_171 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_171 WITH 2.0 WAS PADDED\n",
      "SUBJECT Stan_173 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_173 WITH 2.0 WAS PADDED\n",
      "SUBJECT Stan_173 WITH 5.0 WAS PADDED\n",
      "SUBJECT Stan_173 WITH 6.0 WAS PADDED\n",
      "SUBJECT Stan_173 WITH 7.0 WAS PADDED\n",
      "SUBJECT Stan_176 WITH 6.0 WAS PADDED\n",
      "SUBJECT Stan_176 WITH 13.0 WAS PADDED\n",
      "SUBJECT Stan_176 WITH 16.0 WAS PADDED\n",
      "SUBJECT Stan_177 WITH 2.0 WAS PADDED\n",
      "SUBJECT Stan_177 WITH 3.0 WAS PADDED\n",
      "SUBJECT Stan_177 WITH 4.0 WAS PADDED\n",
      "SUBJECT Stan_183 WITH 2.0 WAS PADDED\n",
      "SUBJECT Stan_184 WITH 2.0 WAS PADDED\n",
      "SUBJECT Stan_185 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_185 WITH 2.0 WAS PADDED\n",
      "SUBJECT Stan_185 WITH 4.0 WAS PADDED\n",
      "SUBJECT Stan_197 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_197 WITH 2.0 WAS PADDED\n",
      "SUBJECT Stan_197 WITH 7.0 WAS PADDED\n",
      "SUBJECT Stan_203 WITH 3.0 WAS PADDED\n",
      "SUBJECT Stan_203 WITH 8.0 WAS PADDED\n",
      "SUBJECT Stan_203 WITH 9.0 WAS PADDED\n",
      "SUBJECT Stan_203 WITH 10.0 WAS PADDED\n",
      "SUBJECT Stan_203 WITH 12.0 WAS PADDED\n",
      "SUBJECT Stan_203 WITH 13.0 WAS PADDED\n",
      "SUBJECT Stan_203 WITH 17.0 WAS PADDED\n",
      "SUBJECT Stan_227 WITH 2.0 WAS PADDED\n",
      "SUBJECT Oslo01 WITH 1.0 WAS PADDED\n",
      "SUBJECT Oslo01 WITH 2.0 WAS PADDED\n",
      "SUBJECT Stan_244 WITH 2.0 WAS PADDED\n",
      "SUBJECT Stan_244 WITH 4.0 WAS PADDED\n",
      "SUBJECT Stan_244 WITH 5.0 WAS PADDED\n",
      "SUBJECT Stan_244 WITH 8.0 WAS PADDED\n",
      "SUBJECT Stan_244 WITH 11.0 WAS PADDED\n",
      "SUBJECT Stan_246 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_246 WITH 3.0 WAS PADDED\n",
      "SUBJECT Stan_251 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_251 WITH 2.0 WAS PADDED\n",
      "SUBJECT Stan_251 WITH 3.0 WAS PADDED\n",
      "SUBJECT Stan_251 WITH 4.0 WAS PADDED\n",
      "SUBJECT Stan_252 WITH 19.0 WAS PADDED\n",
      "SUBJECT Stan_252 WITH 32.0 WAS PADDED\n",
      "SUBJECT Stan_252 WITH 38.0 WAS PADDED\n",
      "SUBJECT Stan_252 WITH 44.0 WAS PADDED\n",
      "SUBJECT Stan_252 WITH 55.0 WAS PADDED\n",
      "SUBJECT Stan_252 WITH 76.0 WAS PADDED\n",
      "SUBJECT Stan_257 WITH 4.0 WAS PADDED\n",
      "SUBJECT Stan_257 WITH 5.0 WAS PADDED\n",
      "SUBJECT Stan_257 WITH 8.0 WAS PADDED\n",
      "SUBJECT Stan_260 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_260 WITH 2.0 WAS PADDED\n",
      "SUBJECT Stan_265 WITH 5.0 WAS PADDED\n",
      "SUBJECT Stan_266 WITH 9.0 WAS PADDED\n",
      "SUBJECT Stan_266 WITH 11.0 WAS PADDED\n",
      "SUBJECT Stan_266 WITH 14.0 WAS PADDED\n",
      "SUBJECT Stan_266 WITH 30.0 WAS PADDED\n",
      "SUBJECT Stan_266 WITH 31.0 WAS PADDED\n",
      "SUBJECT Stan_266 WITH 43.0 WAS PADDED\n",
      "SUBJECT Stan_266 WITH 49.0 WAS PADDED\n",
      "SUBJECT Stan_266 WITH 53.0 WAS PADDED\n",
      "SUBJECT Stan_266 WITH 55.0 WAS PADDED\n",
      "SUBJECT Stan_266 WITH 66.0 WAS PADDED\n",
      "SUBJECT Stan_266 WITH 71.0 WAS PADDED\n",
      "SUBJECT Stan_274 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_283 WITH 3.0 WAS PADDED\n",
      "SUBJECT Stan_283 WITH 14.0 WAS PADDED\n",
      "SUBJECT Stan_283 WITH 36.0 WAS PADDED\n",
      "SUBJECT Stan_283 WITH 56.0 WAS PADDED\n",
      "SUBJECT Stan_283 WITH 61.0 WAS PADDED\n",
      "SUBJECT Stan_283 WITH 62.0 WAS PADDED\n",
      "SUBJECT Stan_283 WITH 69.0 WAS PADDED\n",
      "SUBJECT Stan_283 WITH 75.0 WAS PADDED\n",
      "SUBJECT Stan_283 WITH 82.0 WAS PADDED\n",
      "SUBJECT Stan_285 WITH 2.0 WAS PADDED\n",
      "SUBJECT Stan_289 WITH 3.0 WAS PADDED\n",
      "SUBJECT Stan_289 WITH 4.0 WAS PADDED\n",
      "SUBJECT Stan_289 WITH 6.0 WAS PADDED\n",
      "SUBJECT Stan_289 WITH 9.0 WAS PADDED\n",
      "SUBJECT Stan_289 WITH 15.0 WAS PADDED\n",
      "SUBJECT Stan_291 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_297 WITH 2.0 WAS PADDED\n",
      "SUBJECT Stan_303 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_311 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_312 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_313 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_313 WITH 3.0 WAS PADDED\n",
      "SUBJECT Stan_313 WITH 4.0 WAS PADDED\n",
      "SUBJECT Stan_313 WITH 7.0 WAS PADDED\n",
      "SUBJECT Stan_313 WITH 10.0 WAS PADDED\n",
      "SUBJECT Stan_313 WITH 12.0 WAS PADDED\n",
      "SUBJECT Stan_313 WITH 13.0 WAS PADDED\n",
      "SUBJECT Stan_315 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_315 WITH 4.0 WAS PADDED\n",
      "SUBJECT Stan_315 WITH 5.0 WAS PADDED\n",
      "SUBJECT Stan_315 WITH 13.0 WAS PADDED\n",
      "SUBJECT Stan_318 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_318 WITH 7.0 WAS PADDED\n",
      "SUBJECT Stan_318 WITH 9.0 WAS PADDED\n",
      "SUBJECT Stan_319 WITH 2.0 WAS PADDED\n",
      "SUBJECT Stan_319 WITH 4.0 WAS PADDED\n",
      "SUBJECT Stan_319 WITH 12.0 WAS PADDED\n",
      "SUBJECT Stan_319 WITH 13.0 WAS PADDED\n",
      "SUBJECT Stan_323 WITH 3.0 WAS PADDED\n",
      "SUBJECT Stan_328 WITH 1.0 WAS PADDED\n",
      "SUBJECT Stan_328 WITH 8.0 WAS PADDED\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "seg_dir = '/data/projects/TMOR/data/Deeplearning/'\n",
    "\n",
    "D_x_max, D_y_max, D_z_max = 0, 0, 0\n",
    "\n",
    "for subj_id in os.listdir(seg_dir):\n",
    "    seg_glob = os.path.join(seg_dir, f'{subj_id}', 'seg.nii.gz')\n",
    "    reg_glob = os.path.join(seg_dir, f'{subj_id}', 't1_gd.nii.gz')\n",
    "    seg_paths = glob.glob(seg_glob)\n",
    "    reg_paths = glob.glob(reg_glob)\n",
    "\n",
    "    for seg, reg in zip(seg_paths, reg_paths):\n",
    "        if os.path.exists(seg) and os.path.exists(reg):\n",
    "            mask = nib.load(seg)\n",
    "            vol_mask = mask.get_fdata()\n",
    "\n",
    "            for label in np.unique(vol_mask):\n",
    "                if label != 0:\n",
    "                    label_indices = np.argwhere(vol_mask == label)\n",
    "                    if len(label_indices) > 125:\n",
    "                        indices = label_indices\n",
    "                        X_min, X_max = np.min(indices[:, 0]), np.max(indices[:, 0])\n",
    "                        Y_min, Y_max = np.min(indices[:, 1]), np.max(indices[:, 1])\n",
    "                        Z_min, Z_max = np.min(indices[:, 2]), np.max(indices[:, 2])\n",
    "\n",
    "                        X_avg = (X_min + X_max) // 2\n",
    "                        Y_avg = (Y_min + Y_max) // 2\n",
    "                        Z_avg = (Z_min + Z_max) // 2\n",
    "\n",
    "                        D_x = X_max - X_min\n",
    "                        D_y = Y_max - Y_min\n",
    "                        D_z = Z_max - Z_min\n",
    "\n",
    "                        if D_x_max < D_x:\n",
    "                            D_x_max = D_x\n",
    "                        if D_y_max < D_y:\n",
    "                            D_y_max = D_y\n",
    "                        if D_z_max < D_z:\n",
    "                            D_z_max = D_z\n",
    "\n",
    "                        X_low = max(X_avg - (D_x_max // 2), 0)\n",
    "                        X_high = min(X_avg + (D_x_max // 2), vol_mask.shape[0])\n",
    "                        Y_low = max(Y_avg - (D_y_max // 2), 0)\n",
    "                        Y_high = min(Y_avg + (D_y_max // 2), vol_mask.shape[1])\n",
    "                        Z_low = max(Z_avg - (D_z_max // 2), 0)\n",
    "                        Z_high = min(Z_avg + (D_z_max // 2), vol_mask.shape[2])\n",
    "\n",
    "                        pene = vol_mask[X_low:X_high, Y_low:Y_high, Z_low:Z_high]\n",
    "\n",
    "                        pad_width = [\n",
    "                            (0, max(D_x_max - pene.shape[0], 0)),\n",
    "                            (0, max(D_y_max - pene.shape[1], 0)),\n",
    "                            (0, max(D_z_max - pene.shape[2], 0))\n",
    "                        ]\n",
    "\n",
    "                        seg_partitioned = np.pad(pene, pad_width, mode='constant', constant_values=0)\n",
    "\n",
    "                        if seg_partitioned.shape != pene.shape:\n",
    "                            print('SUBJECT', f'{subj_id}', 'WITH', f'{label}', 'WAS PADDED')\n",
    "\n",
    "                        output_dir = os.path.join(seg_dir, 'C', subj_id)\n",
    "                        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "                        output_path = os.path.join(output_dir, f'{label}_seg.nii.gz')\n",
    "                        nib.save(nib.Nifti1Image(seg_partitioned, mask.affine), output_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T13:00:27.934788Z",
     "end_time": "2023-07-06T13:01:16.534982Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134, 169, 141)\n",
      "(152, 186, 143)\n",
      "(145, 176, 140)\n",
      "(133, 152, 126)\n",
      "(138, 164, 137)\n",
      "(132, 167, 137)\n",
      "(130, 169, 139)\n",
      "(143, 164, 142)\n",
      "(142, 177, 141)\n",
      "(157, 184, 148)\n",
      "(138, 167, 138)\n",
      "(126, 166, 120)\n",
      "(138, 182, 141)\n",
      "(142, 176, 135)\n",
      "(143, 170, 141)\n",
      "(138, 166, 125)\n",
      "(139, 170, 132)\n",
      "(142, 170, 136)\n",
      "(138, 174, 132)\n",
      "(130, 156, 132)\n",
      "(140, 163, 137)\n",
      "(149, 177, 139)\n",
      "(132, 151, 129)\n",
      "(132, 162, 132)\n",
      "(135, 162, 127)\n",
      "(140, 160, 135)\n",
      "(140, 171, 125)\n",
      "(140, 163, 141)\n",
      "(130, 169, 133)\n",
      "(143, 165, 130)\n",
      "(142, 154, 135)\n",
      "(139, 169, 140)\n",
      "(148, 168, 143)\n",
      "(140, 159, 137)\n",
      "(134, 167, 134)\n",
      "(138, 169, 132)\n",
      "(139, 166, 135)\n",
      "(134, 160, 129)\n",
      "(143, 179, 143)\n",
      "(144, 185, 140)\n",
      "(130, 176, 136)\n",
      "(135, 165, 128)\n",
      "(137, 175, 129)\n",
      "(147, 169, 131)\n",
      "(139, 168, 135)\n",
      "(138, 172, 132)\n",
      "(132, 163, 131)\n",
      "(147, 171, 140)\n",
      "(145, 177, 147)\n",
      "(146, 173, 143)\n",
      "(142, 170, 135)\n",
      "(141, 180, 140)\n",
      "(124, 162, 129)\n",
      "(150, 180, 148)\n",
      "(148, 175, 143)\n",
      "(130, 182, 137)\n",
      "(143, 179, 136)\n",
      "(138, 160, 143)\n",
      "(131, 159, 133)\n",
      "(133, 157, 135)\n",
      "(130, 166, 128)\n",
      "(130, 161, 126)\n",
      "(135, 159, 132)\n",
      "(135, 164, 132)\n",
      "(141, 165, 133)\n",
      "(132, 171, 128)\n",
      "(133, 159, 121)\n",
      "(133, 171, 128)\n",
      "(139, 154, 136)\n",
      "(127, 172, 136)\n",
      "(136, 166, 137)\n",
      "(132, 165, 128)\n",
      "(143, 162, 136)\n",
      "(135, 163, 130)\n",
      "(123, 159, 135)\n",
      "(133, 164, 129)\n",
      "(130, 154, 142)\n",
      "(141, 152, 141)\n",
      "(125, 160, 132)\n",
      "(130, 162, 128)\n",
      "(135, 175, 138)\n",
      "(142, 163, 127)\n",
      "(130, 157, 130)\n",
      "(130, 160, 130)\n",
      "(139, 155, 129)\n",
      "(137, 168, 134)\n",
      "(127, 179, 132)\n",
      "(133, 166, 133)\n",
      "(127, 179, 140)\n",
      "(133, 154, 143)\n",
      "(138, 147, 134)\n",
      "(145, 157, 136)\n",
      "(140, 158, 133)\n",
      "(145, 169, 140)\n",
      "(127, 163, 122)\n",
      "(133, 152, 134)\n",
      "(129, 168, 136)\n",
      "(127, 156, 127)\n",
      "(143, 161, 138)\n",
      "(127, 159, 138)\n",
      "(140, 157, 128)\n",
      "(139, 166, 138)\n",
      "(138, 157, 134)\n",
      "(133, 151, 125)\n",
      "(136, 176, 136)\n",
      "(129, 179, 140)\n",
      "(133, 166, 131)\n",
      "(136, 180, 146)\n",
      "(132, 170, 131)\n",
      "(127, 169, 137)\n",
      "(142, 184, 137)\n",
      "(137, 163, 136)\n",
      "(138, 176, 125)\n",
      "(133, 166, 129)\n",
      "(136, 175, 136)\n",
      "(147, 157, 130)\n",
      "(148, 157, 130)\n",
      "(136, 167, 137)\n",
      "(148, 157, 140)\n",
      "(135, 162, 138)\n",
      "(131, 159, 125)\n",
      "(135, 145, 135)\n",
      "(138, 160, 141)\n",
      "(136, 160, 130)\n",
      "(146, 179, 139)\n",
      "(130, 170, 148)\n",
      "(133, 157, 133)\n",
      "(128, 160, 145)\n",
      "(127, 162, 133)\n",
      "(141, 173, 151)\n",
      "(125, 161, 134)\n",
      "(130, 160, 130)\n",
      "(137, 154, 133)\n",
      "(127, 162, 126)\n",
      "(129, 161, 138)\n",
      "(131, 167, 124)\n",
      "(135, 158, 134)\n",
      "(138, 156, 137)\n",
      "(136, 173, 137)\n",
      "(145, 185, 132)\n",
      "(143, 160, 137)\n",
      "(130, 166, 129)\n",
      "(131, 167, 133)\n",
      "(136, 157, 126)\n",
      "(134, 163, 127)\n",
      "(138, 173, 138)\n",
      "(131, 177, 131)\n",
      "(131, 159, 136)\n",
      "(140, 159, 138)\n",
      "(143, 147, 130)\n",
      "(141, 150, 132)\n",
      "(144, 164, 134)\n",
      "(151, 169, 134)\n",
      "(131, 180, 146)\n",
      "(144, 181, 142)\n",
      "(135, 152, 138)\n",
      "(127, 162, 130)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def clamp(num, mn, mx):\n",
    "    \"\"\"\n",
    "    Clamps a number between two values.\n",
    "    :param num: The number to clamp.\n",
    "    :param mn: The minimum value.\n",
    "    :param mx: The maximum value.\n",
    "    :return: The clamped number.\n",
    "    \"\"\"\n",
    "    return max(min(num, mx), mn)\n",
    "\n",
    "\n",
    "seg_dir = '/data/projects/TMOR/data/Deeplearning/'\n",
    "#indices = []\n",
    "for subj_id in os.listdir(seg_dir):\n",
    "    #extrasct the correct id from the subjects\n",
    "    # Define paths to the segmentation and regular t1 postprocessed files for this subject\n",
    "    seg_glob = os.path.join(seg_dir, f'{subj_id}', f'seg.nii.gz')\n",
    "    reg_glob = os.path.join(seg_dir, f'{subj_id}', f't1_gd.nii.gz')\n",
    "    seg_paths = glob.glob(seg_glob)\n",
    "    reg_paths = glob.glob(reg_glob)\n",
    "    # Loop through all matching segmentation and registration files\n",
    "    for seg, reg in zip(seg_paths, reg_paths):\n",
    "        if os.path.exists(seg) and os.path.exists(reg):\n",
    "            mask = nib.load(seg)\n",
    "            vol_mask = mask.get_fdata()\n",
    "            for label in np.unique(vol_mask):\n",
    "                if label != 0:\n",
    "                    label_indices = np.argwhere(vol_mask == label)\n",
    "                    if len(label_indices) > 125:\n",
    "                        indices = label_indices\n",
    "                        X_min, X_max = np.min(indices[:, 0]), np.max(indices[:, 0])\n",
    "                        Y_min, Y_max = np.min(indices[:, 1]), np.max(indices[:, 1])\n",
    "                        Z_min, Z_max = np.min(indices[:, 2]), np.max(indices[:, 2])\n",
    "\n",
    "                        mean_x = (X_min + X_max) // 2\n",
    "                        mean_y = (Y_min + Y_max) // 2\n",
    "                        mean_z = (Z_min + Z_max) // 2\n",
    "                        ###############################\n",
    "                        d_x = D_x_max\n",
    "                        d_y = D_y_max\n",
    "                        d_z = D_z_max\n",
    "                        ##################################\n",
    "                        x_start = clamp(mean_x - d_x // 2, 0, vol_mask.shape[0])\n",
    "                        x_end = clamp(mean_x + d_x // 2, 0, vol_mask.shape[0])\n",
    "\n",
    "                        y_start = clamp(mean_y - d_y // 2, 0, vol_mask.shape[1])\n",
    "                        y_end = clamp(mean_y + d_y // 2, 0, vol_mask.shape[1])\n",
    "\n",
    "                        z_start = clamp(mean_z - d_z // 2, 0, vol_mask.shape[2])\n",
    "                        z_end = clamp(mean_z + d_z // 2, 0, vol_mask.shape[2])\n",
    "\n",
    "                        tumor_cutout = vol_mask[x_start:x_end, y_start:y_end, z_start:z_end]\n",
    "                        ###################################\n",
    "                        tensor = np.zeros([d_x, d_y, d_z])\n",
    "                        # Define the position of the bounding box in the tensor\n",
    "                        tx_start = abs(mean_x - d_x // 2) if mean_x - d_x // 2 < 0 else 0\n",
    "                        tx_end = tumor_cutout.shape[0] + tx_start\n",
    "\n",
    "                        ty_start = abs(mean_y - d_y // 2) if mean_y - d_y // 2 < 0 else 0\n",
    "                        ty_end = tumor_cutout.shape[1] + ty_start\n",
    "\n",
    "                        tz_start = abs(mean_z - d_z // 2) if mean_z - d_z // 2 < 0 else 0\n",
    "                        tz_end = tumor_cutout.shape[2] + tz_start\n",
    "\n",
    "                        # Place the bounding box in the tensor\n",
    "                        tensor[tx_start:tx_end, ty_start:ty_end, tz_start:tz_end] = tumor_cutout\n",
    "                        output_dir = os.path.join(seg_dir, 'B', subj_id)\n",
    "                        os.makedirs(output_dir, exist_ok=True)\n",
    "                         # Save the seg_partitioned into a file\n",
    "                        output_path = os.path.join(output_dir, f'{label}seg_seg_.nii.gz')\n",
    "                        nib.save(nib.Nifti1Image(tumor_cutout, mask.affine), output_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T12:35:46.117116Z",
     "end_time": "2023-07-06T12:36:35.239395Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "outputs": [
    {
     "ename": "ImageFileError",
     "evalue": "Cannot work out file type of \"/data/projects/TMOR/data/Deeplearning/A/Oslo02/seg_seg_.f6.0nii.gz\"",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImageFileError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[427], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m  \u001B[38;5;66;03m# Save the seg_partitioned into a file\u001B[39;00m\n\u001B[1;32m      4\u001B[0m output_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(output_dir, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mseg_seg_.f\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlabel\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mnii.gz\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 5\u001B[0m \u001B[43mnib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mNifti1Image\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseg_partitioned\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maffine\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_path\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/site-packages/nibabel/loadsave.py:179\u001B[0m, in \u001B[0;36msave\u001B[0;34m(img, filename, **kwargs)\u001B[0m\n\u001B[1;32m    177\u001B[0m valid_klasses \u001B[38;5;241m=\u001B[39m [klass \u001B[38;5;28;01mfor\u001B[39;00m klass \u001B[38;5;129;01min\u001B[39;00m all_image_classes \u001B[38;5;28;01mif\u001B[39;00m ext \u001B[38;5;129;01min\u001B[39;00m klass\u001B[38;5;241m.\u001B[39mvalid_exts]\n\u001B[1;32m    178\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m valid_klasses:  \u001B[38;5;66;03m# if list is empty\u001B[39;00m\n\u001B[0;32m--> 179\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ImageFileError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCannot work out file type of \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfilename\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    181\u001B[0m \u001B[38;5;66;03m# Got a list of valid extensions, but that's no guarantee\u001B[39;00m\n\u001B[1;32m    182\u001B[0m \u001B[38;5;66;03m#   the file conversion will work. So, try each image\u001B[39;00m\n\u001B[1;32m    183\u001B[0m \u001B[38;5;66;03m#   in order...\u001B[39;00m\n\u001B[1;32m    184\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m klass \u001B[38;5;129;01min\u001B[39;00m valid_klasses:\n",
      "\u001B[0;31mImageFileError\u001B[0m: Cannot work out file type of \"/data/projects/TMOR/data/Deeplearning/A/Oslo02/seg_seg_.f6.0nii.gz\""
     ]
    }
   ],
   "source": [
    "output_dir = os.path.join(seg_dir, 'A', subj_id)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    " # Save the seg_partitioned into a file\n",
    "output_path = os.path.join(output_dir, f'seg_seg_.nii.gz')\n",
    "nib.save(nib.Nifti1Image(seg_partitioned, mask.affine), output_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-21T17:00:30.420362Z",
     "end_time": "2023-06-21T17:00:30.921783Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "data": {
      "text/plain": "label                     1.000000\nshape_Elongation          0.860610\nshape_Flatness            0.856119\nshape_LeastAxisLength     7.661783\nshape_MajorAxisLength     8.949440\n                           ...    \nAge                      78.000000\nnumber_annotations        1.000000\nF                         1.000000\nM                         0.000000\nlabels                    0.000000\nName: Oslo66, Length: 113, dtype: float64"
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.loc['Oslo66']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T15:17:13.993330Z",
     "end_time": "2023-07-06T15:17:14.033438Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350\n"
     ]
    }
   ],
   "source": [
    "class BrainSegmentationDataset(Dataset):\n",
    "    def __init__(self, img_dir, labels_file, transform=None, target_transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_labels = pd.read_excel(labels_file)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = self.split_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.transform is None:\n",
    "            # Load the mask directly\n",
    "            seg_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0], 'seg.nii.gz')\n",
    "            img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0], 't1_gd.nii.gz')\n",
    "            label = self.y_train[idx]\n",
    "            data = nib.load(img_path).get_fdata()\n",
    "            mask = nib.load(seg_path).get_fdata()\n",
    "        else:\n",
    "            # Apply the transformation to the mask\n",
    "            seg_path = os.path.join(self.img_dinr, self.img_labels.iloc[idx, 0], 'seg.nii.gz')\n",
    "            img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0], 't1_gd.nii.gz')\n",
    "            label = self.y_train[idx]\n",
    "            mask = nib.load(seg_path).get_fdata()\n",
    "            data = nib.load(img_path).get_fdata()\n",
    "\n",
    "        mask = torch.from_numpy(mask).unsqueeze(0)\n",
    "        label = torch.tensor(label).long()\n",
    "\n",
    "        return mask, data,label\n",
    "\n",
    "    def split_data(self):\n",
    "        X = self.img_labels.iloc[:, 0].values\n",
    "        y = self.img_labels.iloc[:, 2].values\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)\n",
    "\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "img_dir = '/data/projects/TMOR/data/Deeplearning/'\n",
    "labels_file = 'labels_DL.xlsx'\n",
    "dataset = BrainSegmentationDataset(img_dir, labels_file)\n",
    "\n",
    "# Access the X_train list\n",
    "print(dataset.X_train.size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T19:05:32.452442Z",
     "end_time": "2023-06-19T19:05:32.490471Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "outputs": [],
   "source": [
    "img_dir = '/data/projects/TMOR/data/Deeplearning/'\n",
    "labels_file = 'labels_DL.xlsx'\n",
    "transform = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])\n",
    "\n",
    "dataset = BrainSegmentationDataset(img_dir,labels_file, transform=None)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T19:35:17.695217Z",
     "end_time": "2023-06-19T19:36:00.426919Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 135, 159, 132])"
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = dataset.X_train, dataset.X_test, dataset.y_train, dataset.y_test\n",
    "\n",
    "X_train[6].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T18:08:05.246161Z",
     "end_time": "2023-06-19T18:08:05.294773Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/projects/TMOR/data/Deeplearning/Oslo01/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo01/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo02/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo02/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo03/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo03/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo04/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo06/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo07/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo08/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo09/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo09/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo10/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo11/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo12/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo13/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo14/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo17/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo18/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo18/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo18/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo19/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo20/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo20/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo21/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo22/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo23/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo24/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo24/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo25/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo26/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo26/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo28/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo29/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo30/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo30/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo31/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo31/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo32/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo32/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo33/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo33/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo34/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo35/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo36/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo37/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo37/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo38/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo38/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo39/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo40/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo41/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo41/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo42/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo43/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo43/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo43/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo44/seg.nii.gz\n",
      "/data/projects/TMOR/data/Deeplearning/Oslo45/seg.nii.gz\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[403], line 31\u001B[0m\n\u001B[1;32m     27\u001B[0m     image_tensors\u001B[38;5;241m.\u001B[39mappend((data_tensor, mask_tensor))\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m image_tensors\n\u001B[0;32m---> 31\u001B[0m processed_images \u001B[38;5;241m=\u001B[39m \u001B[43mprocess_images\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels_file\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[403], line 21\u001B[0m, in \u001B[0;36mprocess_images\u001B[0;34m(img_dir, labels_file)\u001B[0m\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;28mprint\u001B[39m(seg_path)\n\u001B[1;32m     20\u001B[0m     img_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(img_dir, image_name, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt1_gd.nii.gz\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 21\u001B[0m     mask \u001B[38;5;241m=\u001B[39m \u001B[43mnib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseg_path\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_fdata\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m     data \u001B[38;5;241m=\u001B[39m nib\u001B[38;5;241m.\u001B[39mload(img_path)\u001B[38;5;241m.\u001B[39mget_fdata()\n\u001B[1;32m     24\u001B[0m data_tensor \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(data)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/site-packages/nibabel/dataobj_images.py:368\u001B[0m, in \u001B[0;36mDataobjImage.get_fdata\u001B[0;34m(self, caching, dtype)\u001B[0m\n\u001B[1;32m    364\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fdata_cache\n\u001B[1;32m    365\u001B[0m \u001B[38;5;66;03m# Always return requested data type\u001B[39;00m\n\u001B[1;32m    366\u001B[0m \u001B[38;5;66;03m# For array proxies, will attempt to confine data array to dtype\u001B[39;00m\n\u001B[1;32m    367\u001B[0m \u001B[38;5;66;03m# during scaling\u001B[39;00m\n\u001B[0;32m--> 368\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43masanyarray\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    369\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m caching \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfill\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    370\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fdata_cache \u001B[38;5;241m=\u001B[39m data\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/site-packages/nibabel/arrayproxy.py:426\u001B[0m, in \u001B[0;36mArrayProxy.__array__\u001B[0;34m(self, dtype)\u001B[0m\n\u001B[1;32m    405\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__array__\u001B[39m(\u001B[38;5;28mself\u001B[39m, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    406\u001B[0m     \u001B[38;5;124;03m\"\"\"Read data from file and apply scaling, casting to ``dtype``\u001B[39;00m\n\u001B[1;32m    407\u001B[0m \n\u001B[1;32m    408\u001B[0m \u001B[38;5;124;03m    If ``dtype`` is unspecified, the dtype of the returned array is the\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    424\u001B[0m \u001B[38;5;124;03m        Scaled image data with type `dtype`.\u001B[39;00m\n\u001B[1;32m    425\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 426\u001B[0m     arr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_scaled\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mslicer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    427\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    428\u001B[0m         arr \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mastype(dtype, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/site-packages/nibabel/arrayproxy.py:393\u001B[0m, in \u001B[0;36mArrayProxy._get_scaled\u001B[0;34m(self, dtype, slicer)\u001B[0m\n\u001B[1;32m    391\u001B[0m     scl_inter \u001B[38;5;241m=\u001B[39m scl_inter\u001B[38;5;241m.\u001B[39mastype(use_dtype)\n\u001B[1;32m    392\u001B[0m \u001B[38;5;66;03m# Read array and upcast as necessary for big slopes, intercepts\u001B[39;00m\n\u001B[0;32m--> 393\u001B[0m scaled \u001B[38;5;241m=\u001B[39m apply_read_scaling(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_unscaled\u001B[49m\u001B[43m(\u001B[49m\u001B[43mslicer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mslicer\u001B[49m\u001B[43m)\u001B[49m, scl_slope, scl_inter)\n\u001B[1;32m    394\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    395\u001B[0m     scaled \u001B[38;5;241m=\u001B[39m scaled\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mpromote_types(scaled\u001B[38;5;241m.\u001B[39mdtype, dtype), copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/site-packages/nibabel/arrayproxy.py:363\u001B[0m, in \u001B[0;36mArrayProxy._get_unscaled\u001B[0;34m(self, slicer)\u001B[0m\n\u001B[1;32m    359\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m canonical_slicers(slicer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shape, \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m==\u001B[39m canonical_slicers(\n\u001B[1;32m    360\u001B[0m     (), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shape, \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    361\u001B[0m ):\n\u001B[1;32m    362\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_fileobj() \u001B[38;5;28;01mas\u001B[39;00m fileobj, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m--> 363\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43marray_from_file\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    364\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_shape\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    365\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    366\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfileobj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    367\u001B[0m \u001B[43m            \u001B[49m\u001B[43moffset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_offset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    368\u001B[0m \u001B[43m            \u001B[49m\u001B[43morder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43morder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    369\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmmap\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_mmap\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    370\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    371\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_fileobj() \u001B[38;5;28;01mas\u001B[39;00m fileobj:\n\u001B[1;32m    372\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fileslice(\n\u001B[1;32m    373\u001B[0m         fileobj,\n\u001B[1;32m    374\u001B[0m         slicer,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    379\u001B[0m         lock\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock,\n\u001B[1;32m    380\u001B[0m     )\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/site-packages/nibabel/volumeutils.py:454\u001B[0m, in \u001B[0;36marray_from_file\u001B[0;34m(shape, in_dtype, infile, offset, order, mmap)\u001B[0m\n\u001B[1;32m    452\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(infile, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreadinto\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m    453\u001B[0m     data_bytes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mbytearray\u001B[39m(n_bytes)\n\u001B[0;32m--> 454\u001B[0m     n_read \u001B[38;5;241m=\u001B[39m \u001B[43minfile\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadinto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_bytes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    455\u001B[0m     needs_copy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    456\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/gzip.py:300\u001B[0m, in \u001B[0;36mGzipFile.read\u001B[0;34m(self, size)\u001B[0m\n\u001B[1;32m    298\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01merrno\u001B[39;00m\n\u001B[1;32m    299\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(errno\u001B[38;5;241m.\u001B[39mEBADF, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mread() on write-only GzipFile object\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 300\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_buffer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43msize\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/_compression.py:68\u001B[0m, in \u001B[0;36mDecompressReader.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreadinto\u001B[39m(\u001B[38;5;28mself\u001B[39m, b):\n\u001B[1;32m     67\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mmemoryview\u001B[39m(b) \u001B[38;5;28;01mas\u001B[39;00m view, view\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mB\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m byte_view:\n\u001B[0;32m---> 68\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mbyte_view\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     69\u001B[0m         byte_view[:\u001B[38;5;28mlen\u001B[39m(data)] \u001B[38;5;241m=\u001B[39m data\n\u001B[1;32m     70\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(data)\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/gzip.py:509\u001B[0m, in \u001B[0;36m_GzipReader.read\u001B[0;34m(self, size)\u001B[0m\n\u001B[1;32m    505\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m buf \u001B[38;5;241m==\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    506\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEOFError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCompressed file ended before the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    507\u001B[0m                        \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mend-of-stream marker was reached\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 509\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_add_read_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43muncompress\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    510\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pos \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(uncompress)\n\u001B[1;32m    511\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m uncompress\n",
      "File \u001B[0;32m~/.conda/envs/secondenv/lib/python3.9/gzip.py:514\u001B[0m, in \u001B[0;36m_GzipReader._add_read_data\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m    513\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_add_read_data\u001B[39m(\u001B[38;5;28mself\u001B[39m, data):\n\u001B[0;32m--> 514\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_crc \u001B[38;5;241m=\u001B[39m \u001B[43mzlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcrc32\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_crc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    515\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stream_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stream_size \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mlen\u001B[39m(data)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame called \"data\"\n",
    "# Features are stored in columns X1, X2, X3, ...\n",
    "# The target variable is stored in column 'y'\n",
    "\n",
    "X = new_df.drop(['Labels', 'Tumor'],axis=1)  # Features\n",
    "y = new_df['Labels']  # Target variable\n",
    "# Random split into 80% train and 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train\n",
    "\n",
    "img_dir = '/data/projects/TMOR/data/Deeplearning/'\n",
    "labels_file = pd.read_excel('labels_DL.xlsx')\n",
    "\n",
    "def process_images(img_dir, labels_file):\n",
    "    image_tensors = []\n",
    "    pene = list(labels_file.iloc[:, 0])\n",
    "    for image_name in pene:\n",
    "        seg_path = os.path.join(img_dir, image_name, 'seg.nii.gz')\n",
    "        print(seg_path)\n",
    "        img_path = os.path.join(img_dir, image_name, 't1_gd.nii.gz')\n",
    "        mask = nib.load(seg_path).get_fdata()\n",
    "        data = nib.load(img_path).get_fdata()\n",
    "\n",
    "    data_tensor = torch.from_numpy(data).unsqueeze(0)\n",
    "    mask_tensor = torch.from_numpy(mask).unsqueeze(0)\n",
    "\n",
    "    image_tensors.append((data_tensor, mask_tensor))\n",
    "\n",
    "    return image_tensors\n",
    "\n",
    "processed_images = process_images(img_dir, labels_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T18:44:25.135265Z",
     "end_time": "2023-06-19T18:44:55.162618Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stan_265\n",
      "Oslo62\n",
      "Stan_197\n",
      "Stan_058\n",
      "Stan_319\n",
      "Oslo47\n",
      "Stan_026\n",
      "Stan_165\n",
      "Oslo31\n",
      "Stan_260\n",
      "Stan_049\n",
      "Stan_312\n",
      "Oslo41\n",
      "Stan_045\n",
      "Oslo26\n",
      "Stan_251\n",
      "Stan_285\n",
      "Oslo13\n",
      "Stan_051\n",
      "Stan_028\n",
      "Stan_183\n",
      "Oslo32\n",
      "Stan_064\n",
      "Stan_203\n",
      "Oslo46\n",
      "Oslo06\n",
      "Oslo37\n",
      "Stan_009\n",
      "Stan_315\n",
      "Stan_024\n",
      "Oslo20\n",
      "Stan_184\n",
      "Oslo12\n",
      "Stan_126\n",
      "Stan_120\n",
      "Oslo01\n",
      "Stan_144\n",
      "Stan_019\n",
      "Oslo49\n",
      "Stan_054\n",
      "Oslo33\n",
      "Oslo45\n",
      "Stan_291\n",
      "Oslo30\n",
      "Oslo28\n",
      "Stan_134\n",
      "Oslo44\n",
      "Stan_059\n",
      "Stan_102\n",
      "Stan_318\n",
      "Stan_246\n",
      "Oslo52\n",
      "Stan_148\n",
      "Stan_010\n",
      "Stan_038\n",
      "Oslo38\n",
      "Stan_121\n",
      "Stan_136\n",
      "Stan_171\n",
      "Stan_014\n",
      "Stan_087\n",
      "Stan_098\n",
      "Oslo60\n",
      "Oslo07\n",
      "Stan_132\n",
      "Stan_170\n",
      "Oslo54\n",
      "Oslo40\n",
      "Stan_052\n",
      "Stan_047\n",
      "Oslo39\n",
      "Oslo51\n",
      "Oslo09\n",
      "Oslo48\n",
      "Stan_033\n",
      "Stan_307\n",
      "Stan_072\n",
      "Stan_055\n",
      "Stan_289\n",
      "Stan_290\n",
      "Stan_068\n",
      "Oslo10\n",
      "Stan_142\n",
      "Oslo17\n",
      "Stan_005\n",
      "Stan_257\n",
      "Oslo04\n",
      "Oslo21\n",
      "Oslo43\n",
      "Stan_037\n",
      "Stan_266\n",
      "Oslo08\n",
      "Stan_244\n",
      "Stan_101\n",
      "Oslo03\n",
      "Stan_173\n",
      "Stan_016\n",
      "Oslo61\n",
      "Stan_123\n",
      "Oslo55\n",
      "Stan_323\n",
      "Oslo66\n",
      "Oslo53\n",
      "Stan_066\n",
      "Oslo25\n",
      "Oslo65\n",
      "Stan_316\n",
      "Stan_227\n",
      "Oslo42\n",
      "Stan_311\n",
      "Oslo02\n",
      "Oslo59\n",
      "Stan_230\n",
      "Stan_111\n",
      "Stan_100\n",
      "Stan_149\n",
      "Stan_065\n",
      "Stan_039\n",
      "Stan_176\n",
      "Stan_338\n",
      "Oslo24\n",
      "Stan_036\n",
      "Stan_122\n",
      "Oslo18\n",
      "Stan_074\n",
      "Stan_107\n"
     ]
    }
   ],
   "source": [
    "pene = list(X_train.iloc[:, 0])\n",
    "\n",
    "for i in pene:\n",
    "    print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T18:59:48.543659Z",
     "end_time": "2023-06-19T18:59:48.569369Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "outputs": [
    {
     "data": {
      "text/plain": "Subjid    Stan_319\nName: 2369, dtype: object"
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[4, :]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T18:55:18.616208Z",
     "end_time": "2023-06-19T18:55:18.656019Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
